{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1115.519577</td>\n",
       "      <td>5645.840622</td>\n",
       "      <td>3131.505048</td>\n",
       "      <td>4026.917646</td>\n",
       "      <td>10300.539485</td>\n",
       "      <td>6436.010296</td>\n",
       "      <td>0.184310</td>\n",
       "      <td>0.142384</td>\n",
       "      <td>0.216083</td>\n",
       "      <td>0.223065</td>\n",
       "      <td>0.234158</td>\n",
       "      <td>0.443213</td>\n",
       "      <td>0.119204</td>\n",
       "      <td>0.039594</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.681880</td>\n",
       "      <td>0.288134</td>\n",
       "      <td>0.353661</td>\n",
       "      <td>0.095466</td>\n",
       "      <td>0.756271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1136.724206</td>\n",
       "      <td>6173.405093</td>\n",
       "      <td>1329.314912</td>\n",
       "      <td>19928.490868</td>\n",
       "      <td>40806.007028</td>\n",
       "      <td>24493.111862</td>\n",
       "      <td>0.262799</td>\n",
       "      <td>0.221233</td>\n",
       "      <td>0.282044</td>\n",
       "      <td>0.294870</td>\n",
       "      <td>0.289054</td>\n",
       "      <td>0.116933</td>\n",
       "      <td>0.097157</td>\n",
       "      <td>0.017430</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.190380</td>\n",
       "      <td>0.156324</td>\n",
       "      <td>0.104526</td>\n",
       "      <td>0.071362</td>\n",
       "      <td>0.247798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018183</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.393750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3557.341810</td>\n",
       "      <td>2377.668653</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>980.000000</td>\n",
       "      <td>0.025058</td>\n",
       "      <td>0.025014</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.026685</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.395910</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>0.028374</td>\n",
       "      <td>0.009611</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.306108</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1020.750000</td>\n",
       "      <td>4346.305556</td>\n",
       "      <td>2867.653996</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.033346</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.040986</td>\n",
       "      <td>0.453431</td>\n",
       "      <td>0.118908</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.358929</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2056.403080</td>\n",
       "      <td>6016.750000</td>\n",
       "      <td>3595.493907</td>\n",
       "      <td>2600.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>5166.666667</td>\n",
       "      <td>0.240394</td>\n",
       "      <td>0.151275</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.368949</td>\n",
       "      <td>0.400919</td>\n",
       "      <td>0.508519</td>\n",
       "      <td>0.178048</td>\n",
       "      <td>0.050228</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.411685</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3610.124972</td>\n",
       "      <td>298400.000000</td>\n",
       "      <td>43567.659946</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>0.926994</td>\n",
       "      <td>0.925947</td>\n",
       "      <td>0.919999</td>\n",
       "      <td>0.926534</td>\n",
       "      <td>0.927191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727841</td>\n",
       "      <td>0.155488</td>\n",
       "      <td>0.184932</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         kw_min_avg     kw_max_avg    kw_avg_avg  self_reference_min_shares  \\\n",
       "count  31715.000000   31715.000000  31715.000000               31715.000000   \n",
       "mean    1115.519577    5645.840622   3131.505048                4026.917646   \n",
       "std     1136.724206    6173.405093   1329.314912               19928.490868   \n",
       "min       -1.000000       0.000000      0.000000                   0.000000   \n",
       "25%        0.000000    3557.341810   2377.668653                 642.000000   \n",
       "50%     1020.750000    4346.305556   2867.653996                1200.000000   \n",
       "75%     2056.403080    6016.750000   3595.493907                2600.000000   \n",
       "max     3610.124972  298400.000000  43567.659946              843300.000000   \n",
       "\n",
       "       self_reference_max_shares  self_reference_avg_sharess        LDA_00  \\\n",
       "count               31715.000000                31715.000000  31715.000000   \n",
       "mean                10300.539485                 6436.010296      0.184310   \n",
       "std                 40806.007028                24493.111862      0.262799   \n",
       "min                     0.000000                    0.000000      0.018183   \n",
       "25%                  1100.000000                  980.000000      0.025058   \n",
       "50%                  2800.000000                 2200.000000      0.033387   \n",
       "75%                  7900.000000                 5166.666667      0.240394   \n",
       "max                843300.000000               843300.000000      0.926994   \n",
       "\n",
       "             LDA_01        LDA_02        LDA_03        LDA_04  \\\n",
       "count  31715.000000  31715.000000  31715.000000  31715.000000   \n",
       "mean       0.142384      0.216083      0.223065      0.234158   \n",
       "std        0.221233      0.282044      0.294870      0.289054   \n",
       "min        0.018182      0.018182      0.018182      0.018182   \n",
       "25%        0.025014      0.028571      0.026685      0.028574   \n",
       "50%        0.033346      0.040004      0.040001      0.040986   \n",
       "75%        0.151275      0.333200      0.368949      0.400919   \n",
       "max        0.925947      0.919999      0.926534      0.927191   \n",
       "\n",
       "       global_subjectivity  global_sentiment_polarity  \\\n",
       "count         31715.000000               31715.000000   \n",
       "mean              0.443213                   0.119204   \n",
       "std               0.116933                   0.097157   \n",
       "min               0.000000                  -0.393750   \n",
       "25%               0.395910                   0.057436   \n",
       "50%               0.453431                   0.118908   \n",
       "75%               0.508519                   0.178048   \n",
       "max               1.000000                   0.727841   \n",
       "\n",
       "       global_rate_positive_words  global_rate_negative_words  \\\n",
       "count                31715.000000                31715.000000   \n",
       "mean                     0.039594                    0.016627   \n",
       "std                      0.017430                    0.010892   \n",
       "min                      0.000000                    0.000000   \n",
       "25%                      0.028374                    0.009611   \n",
       "50%                      0.038961                    0.015332   \n",
       "75%                      0.050228                    0.021739   \n",
       "max                      0.155488                    0.184932   \n",
       "\n",
       "       rate_positive_words  rate_negative_words  avg_positive_polarity  \\\n",
       "count         31715.000000         31715.000000           31715.000000   \n",
       "mean              0.681880             0.288134               0.353661   \n",
       "std               0.190380             0.156324               0.104526   \n",
       "min               0.000000             0.000000               0.000000   \n",
       "25%               0.600000             0.185185               0.306108   \n",
       "50%               0.710526             0.280000               0.358929   \n",
       "75%               0.800000             0.384615               0.411685   \n",
       "max               1.000000             1.000000               1.000000   \n",
       "\n",
       "       min_positive_polarity  max_positive_polarity  \n",
       "count           31715.000000           31715.000000  \n",
       "mean                0.095466               0.756271  \n",
       "std                 0.071362               0.247798  \n",
       "min                 0.000000               0.000000  \n",
       "25%                 0.050000               0.600000  \n",
       "50%                 0.100000               0.800000  \n",
       "75%                 0.100000               1.000000  \n",
       "max                 1.000000               1.000000  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = pd.read_csv('../dataset/development.csv')\n",
    "df_eval = pd.read_csv('../dataset/evaluation.csv')\n",
    "\n",
    "df_dev.iloc[:, 20:40].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcDrop(res):\n",
    "    # All variables with correlation > cutoff\n",
    "    all_corr_vars = list(set(res['v1'].tolist() + res['v2'].tolist()))\n",
    "\n",
    "    # All unique variables in drop column\n",
    "    poss_drop = list(set(res['drop'].tolist()))\n",
    "\n",
    "    # Keep any variable not in drop column\n",
    "    keep = list(set(all_corr_vars).difference(set(poss_drop)))\n",
    "\n",
    "    # Drop any variables in same row as a keep variable\n",
    "    p = res[ res['v1'].isin(keep)  | res['v2'].isin(keep) ][['v1', 'v2']]\n",
    "    q = list(set(p['v1'].tolist() + p['v2'].tolist()))\n",
    "    drop = (list(set(q).difference(set(keep))))\n",
    "\n",
    "    # Remove drop variables from possible drop\n",
    "    poss_drop = list(set(poss_drop).difference(set(drop)))\n",
    "\n",
    "    # subset res dataframe to include possible drop pairs\n",
    "    m = res[ res['v1'].isin(poss_drop)  | res['v2'].isin(poss_drop) ][['v1', 'v2','drop']]\n",
    "\n",
    "    # remove rows that are decided (drop), take set and add to drops\n",
    "    more_drop = set(list(m[~m['v1'].isin(drop) & ~m['v2'].isin(drop)]['drop']))\n",
    "    for item in more_drop:\n",
    "        drop.append(item)\n",
    "\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrX_new(df, cut = 0.9):\n",
    "    # Get correlation matrix and upper triagle\n",
    "    corr_mtx = df.corr().abs()\n",
    "    avg_corr = corr_mtx.mean(axis = 1)\n",
    "    up = corr_mtx.where(np.triu(np.ones(corr_mtx.shape), k=1).astype(np.bool_))\n",
    "    dropcols = list()\n",
    "\n",
    "    res = pd.DataFrame(columns=(['v1', 'v2', 'v1.target',\n",
    "                                 'v2.target','corr', 'drop' ]))\n",
    "    for row in range(len(up)-1):\n",
    "        col_idx = row + 1\n",
    "        for col in range (col_idx, len(up)):\n",
    "            if corr_mtx.iloc[row, col] > cut:\n",
    "                if avg_corr.iloc[row] > avg_corr.iloc[col]:\n",
    "                    dropcols.append(row)\n",
    "                    drop = corr_mtx.columns[row]\n",
    "                else:\n",
    "                    dropcols.append(col)\n",
    "                    drop = corr_mtx.columns[col]\n",
    "\n",
    "                s = pd.Series([ corr_mtx.index[row],\n",
    "                                up.columns[col],\n",
    "                                avg_corr[row],\n",
    "                                avg_corr[col],\n",
    "                                up.iloc[row,col],\n",
    "                                drop],\n",
    "                              index = res.columns)\n",
    "\n",
    "                res.loc[len(res)] = s.to_numpy()\n",
    "\n",
    "    dropcols_names = calcDrop(res)\n",
    "\n",
    "    return dropcols_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_158222/4151814631.py:3: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  corr_mtx = df.corr().abs()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avg_positive_polarity', 'kw_avg_avg', 'self_reference_avg_sharess', 'avg_negative_polarity', 'kw_avg_min', 'kw_max_max', 'rate_negative_words', 'abs_title_sentiment_polarity', 'n_non_stop_unique_tokens', 'rate_positive_words', 'n_non_stop_words']\n"
     ]
    }
   ],
   "source": [
    "drop_new = corrX_new(df_dev, cut = 0.70)\n",
    "print(drop_new)\n",
    "# working_df_dev.drop(drop_new, axis=1, inplace=True)\n",
    "# df_working_df_eval.drop(drop_new, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_preprocessing(df, scaler = None, dev_stats=None):\n",
    "\n",
    "    # one hot encoding\n",
    "    working_df = df.copy()\n",
    "    enc = OneHotEncoder()\n",
    "    encoded_df = pd.concat([working_df['weekday'], working_df['data_channel']], axis=1)\n",
    "    enc.fit(encoded_df)\n",
    "    encoded_df = enc.transform(encoded_df)\n",
    "    additional_columns = enc.get_feature_names_out()\n",
    "    working_df[additional_columns] = encoded_df.toarray()\n",
    "    # print(working_df.shape)\n",
    "\n",
    "    is_weekend = []\n",
    "    for _, row in working_df.iterrows():\n",
    "        if row['weekday_sunday'] == 1 or row['weekday_saturday'] == 1:\n",
    "            is_weekend.append(1)\n",
    "        else:\n",
    "            is_weekend.append(0)\n",
    "    working_df['is_weekend'] = is_weekend\n",
    "    working_df.drop(columns=[x for x in additional_columns if x.startswith('weekday')], inplace=True)\n",
    "\n",
    "    # feature selection from correlation analysis\n",
    "    working_df.drop(['weekday', 'data_channel', 'url', 'id', 'n_tokens_content', 'n_non_stop_words', 'kw_max_min',\n",
    "                'kw_min_max', 'kw_min_min', 'kw_max_avg', 'title_subjectivity', 'rate_positive_words'], axis = 1, inplace=True)\n",
    "    # , 'kw_avg_min', 'kw_avg_avg'\n",
    "    # 'url', 'id','weekday','data_channel'\n",
    "    working_df.drop(['self_reference_avg_sharess', 'kw_max_max', 'avg_positive_polarity', 'rate_negative_words', 'abs_title_sentiment_polarity', 'avg_negative_polarity', 'n_non_stop_unique_tokens'], inplace=True, axis=1)\n",
    "    \n",
    "    # fill missing values\n",
    "    working_df['num_keywords'].fillna(0, inplace=True)\n",
    "    working_df['num_imgs'].fillna(0, inplace=True)\n",
    "    working_df['num_self_hrefs'].fillna(0, inplace=True)\n",
    "    working_df['num_videos'].fillna(0, inplace=True)\n",
    "    if dev_stats == None:\n",
    "        dev_stats = dict()\n",
    "        kw_avg_min_mean =  working_df['kw_avg_min'][working_df['kw_avg_min']>0].mean()\n",
    "        kw_min_avg_mean =  working_df['kw_min_avg'][working_df['kw_min_avg']>0].mean()\n",
    "        working_df['kw_avg_min'] = working_df['kw_avg_min'].apply(lambda x: kw_avg_min_mean if x == -1 else x)\n",
    "        working_df['kw_min_avg'] = working_df['kw_min_avg'].apply(lambda x: kw_min_avg_mean if x == -1 else x)\n",
    "        dev_stats['kw_avg_min_mean'] = kw_avg_min_mean\n",
    "        dev_stats['kw_min_avg_mean'] = kw_min_avg_mean\n",
    "    else:\n",
    "        working_df['kw_avg_min'] = working_df['kw_avg_min'].apply(lambda x: dev_stats['kw_avg_min_mean'] if x == -1 else x)\n",
    "        working_df['kw_min_avg'] = working_df['kw_min_avg'].apply(lambda x: dev_stats['kw_avg_min_mean'] if x == -1 else x)\n",
    "    \n",
    "    \n",
    "    if scaler == None:\n",
    "        q1 = working_df['num_hrefs'].describe()['25%']\n",
    "        q3 = working_df['num_hrefs'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_num_hrefs = q1 - 1.5*iqr\n",
    "        max_num_hrefs = q3 + 1.5*iqr\n",
    "        # print(min_num_hrefs, max_num_hrefs)\n",
    "        working_df = working_df[(df.num_hrefs < max_num_hrefs) & (df.num_hrefs > min_num_hrefs)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['kw_avg_min'].describe()['25%']\n",
    "        q3 = working_df['kw_avg_min'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_kw_avg_min = q1 - 1.5*iqr\n",
    "        max_kw_avg_min = q3 + 1.5*iqr\n",
    "        # print(min_kw_avg_min, max_kw_avg_min)\n",
    "        working_df = working_df[(df.kw_avg_min < max_kw_avg_min) & (df.kw_avg_min > min_kw_avg_min)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "        # q1 = working_df['num_self_hrefs'].describe()['25%']\n",
    "        # q3 = working_df['num_self_hrefs'].describe()['75%']\n",
    "        # iqr = q3 - q1\n",
    "        # min_num_self_hrefs = q1 - 1.5*iqr\n",
    "        # max_num_self_hrefs = q3 + 1.5*iqr\n",
    "        # # print(min_num_self_hrefs, max_num_self_hrefs)\n",
    "        # working_df = working_df[(df.num_self_hrefs < max_num_self_hrefs) & (df.num_self_hrefs > min_num_self_hrefs)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "        # q1 = working_df['num_imgs'].describe()['25%']\n",
    "        # q3 = working_df['num_imgs'].describe()['75%']\n",
    "        # iqr = q3 - q1\n",
    "        # min_num_imgs = q1 - 1.5*iqr\n",
    "        # max_num_imgs = q3 + 1.5*iqr\n",
    "        # # print(min_num_imgs, max_num_imgs)\n",
    "        # working_df = working_df[(df.num_imgs < max_num_imgs) & (df.num_imgs > min_num_imgs)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['num_videos'].describe()['25%']\n",
    "        q3 = working_df['num_videos'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_num_videos = q1 - 1.5*iqr\n",
    "        max_num_videos = q3 + 1.5*iqr\n",
    "        # print(min_num_videos, max_num_videos)\n",
    "        working_df = working_df[(df.num_videos < max_num_videos) & (df.num_videos > min_num_videos)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['kw_avg_avg'].describe()['25%']\n",
    "        q3 = working_df['kw_avg_avg'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_kw_avg_avg = q1 - 1.5*iqr\n",
    "        max_kw_avg_avg = q3 + 1.5*iqr\n",
    "        # print(min_kw_avg_avg, max_kw_avg_avg)\n",
    "        working_df = working_df[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['self_reference_min_shares'].describe()['25%']\n",
    "        q3 = working_df['self_reference_min_shares'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_self_reference_min_shares = q1 - 1.5*iqr\n",
    "        max_self_reference_min_shares = q3 + 1.5*iqr\n",
    "        # print(min_self_reference_min_shares, max_self_reference_min_shares)\n",
    "        working_df = working_df[(df.self_reference_min_shares < max_self_reference_min_shares) & (df.self_reference_min_shares > min_self_reference_min_shares)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['self_reference_max_shares'].describe()['25%']\n",
    "        q3 = working_df['self_reference_max_shares'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_self_reference_max_shares = q1 - 1.5*iqr\n",
    "        max_self_reference_max_shares = q3 + 1.5*iqr\n",
    "        # print(min_self_reference_min_shares, max_self_reference_min_shares)\n",
    "        working_df = working_df[(df.self_reference_max_shares < max_self_reference_max_shares) & (df.self_reference_max_shares > min_self_reference_max_shares)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "    # log scales\n",
    "    working_df['num_imgs'] = np.log(1 + working_df['num_imgs'])\n",
    "    working_df['num_self_hrefs'] = np.log(1 + working_df['num_self_hrefs'])\n",
    "    working_df['kw_avg_avg'] = np.log(1+working_df['kw_avg_avg'])\n",
    "    working_df['kw_avg_min'] =np.log(1+working_df['kw_avg_min'])\n",
    "    # avoid log on num_videos because it makes it worse\n",
    "\n",
    "    y_dev = None\n",
    "    \n",
    "    # standard scaler\n",
    "    if scaler == None:\n",
    "        print(working_df.shape)\n",
    "        # Remove outliers from kw_avg_avg (we lost another 9% of the dataset)\n",
    "        # q1 = working_df['kw_avg_avg'].describe()['25%']\n",
    "        # q3 = working_df['kw_avg_avg'].describe()['75%']\n",
    "        # iqr = q3 - q1\n",
    "        # min_kw_avg_avg = q1 - 1.5*iqr\n",
    "        # max_kw_avg_avg = q3 + 1.5*iqr\n",
    "        # working_df = working_df[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n",
    "\n",
    "        working_df['shares'] = np.log(working_df['shares'])\n",
    "        y_dev = working_df['shares']\n",
    "        working_df.drop(columns=['shares'], inplace=True)\n",
    "        scaler = StandardScaler().fit(working_df)\n",
    "        scaled_features = scaler.transform(working_df)\n",
    "        working_df[:] = scaled_features[:]\n",
    "\n",
    "        trans = RFECV(estimator=XGBRegressor(), step=1, cv=4 ,n_jobs=-1, verbose=2, scoring='neg_root_mean_squared_error')\n",
    "        trans.fit(working_df, y_dev)\n",
    "        dev_stats['trans'] = trans\n",
    "        working_df = trans.transform(working_df)\n",
    "        \n",
    "    else:\n",
    "        scaled_features = scaler.transform(working_df)\n",
    "        working_df[:] = scaled_features[:]\n",
    "        trans = dev_stats['trans']\n",
    "        working_df = trans.transform(working_df)\n",
    "\n",
    "    # print(scaled_features.shape)\n",
    "    \n",
    "   \n",
    "\n",
    "    return working_df, scaler, y_dev, dev_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29820, 38)\n",
      "(28208, 38)\n",
      "(20964, 38)\n",
      "(19986, 38)\n",
      "(17562, 38)\n",
      "(15562, 38)\n",
      "(15562, 38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_158222/100936949.py:63: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.kw_avg_min < max_kw_avg_min) & (df.kw_avg_min > min_kw_avg_min)]\n",
      "/var/tmp/ipykernel_158222/100936949.py:90: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.num_videos < max_num_videos) & (df.num_videos > min_num_videos)]\n",
      "/var/tmp/ipykernel_158222/100936949.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n",
      "/var/tmp/ipykernel_158222/100936949.py:108: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.self_reference_min_shares < max_self_reference_min_shares) & (df.self_reference_min_shares > min_self_reference_min_shares)]\n",
      "/var/tmp/ipykernel_158222/100936949.py:117: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.self_reference_max_shares < max_self_reference_max_shares) & (df.self_reference_max_shares > min_self_reference_max_shares)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 36 features.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gesposito/project/predict-shares-online-news/2_feature_engeneering/correlation_analysis_take2.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blegionlogin.polito.it/home/gesposito/project/predict-shares-online-news/2_feature_engeneering/correlation_analysis_take2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m working_df_dev, std_scaler, y_dev, dev_stats \u001b[39m=\u001b[39m final_preprocessing(df_dev)\n",
      "\u001b[1;32m/home/gesposito/project/predict-shares-online-news/2_feature_engeneering/correlation_analysis_take2.ipynb Cell 8\u001b[0m in \u001b[0;36mfinal_preprocessing\u001b[0;34m(df, scaler, dev_stats)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blegionlogin.polito.it/home/gesposito/project/predict-shares-online-news/2_feature_engeneering/correlation_analysis_take2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m working_df[:] \u001b[39m=\u001b[39m scaled_features[:]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blegionlogin.polito.it/home/gesposito/project/predict-shares-online-news/2_feature_engeneering/correlation_analysis_take2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m trans \u001b[39m=\u001b[39m RFECV(estimator\u001b[39m=\u001b[39mXGBRegressor(), step\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m ,n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mneg_root_mean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Blegionlogin.polito.it/home/gesposito/project/predict-shares-online-news/2_feature_engeneering/correlation_analysis_take2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m trans\u001b[39m.\u001b[39;49mfit(working_df, y_dev)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blegionlogin.polito.it/home/gesposito/project/predict-shares-online-news/2_feature_engeneering/correlation_analysis_take2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m dev_stats[\u001b[39m'\u001b[39m\u001b[39mtrans\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trans\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blegionlogin.polito.it/home/gesposito/project/predict-shares-online-news/2_feature_engeneering/correlation_analysis_take2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m working_df \u001b[39m=\u001b[39m trans\u001b[39m.\u001b[39mtransform(working_df)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/sklearn/feature_selection/_rfe.py:725\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    722\u001b[0m     parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n\u001b[1;32m    723\u001b[0m     func \u001b[39m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m--> 725\u001b[0m scores \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    726\u001b[0m     func(rfe, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator, X, y, train, test, scorer)\n\u001b[1;32m    727\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    728\u001b[0m )\n\u001b[1;32m    730\u001b[0m scores \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(scores)\n\u001b[1;32m    731\u001b[0m scores_sum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(scores, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/concurrent/futures/_base.py:434\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    432\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 434\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    436\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "working_df_dev, std_scaler, y_dev, dev_stats = final_preprocessing(df_dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = working_df_dev\n",
    "y = y_dev\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791491126081701\n",
      "0.13982854310139237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0.7688991495408469\\n0.16894985155235465\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg = GradientBoostingRegressor(**{'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': 4, 'min_samples_split': 2, 'n_estimators': 200, 'random_state': 42})\n",
    "rfreg.fit(X_train, y_train)\n",
    "\n",
    "rms = mean_squared_error(y_valid, rfreg.predict(X_valid), squared=False)\n",
    "print(rms)\n",
    "r2 = r2_score(y_valid, rfreg.predict(X_valid))\n",
    "adj_r2 = 1-(1-r2)*(len(X_valid) - 1)/(len(X_valid) - X_valid.shape[1] - 1)\n",
    "print(adj_r2)\n",
    "'''\n",
    "0.7688991495408469\n",
    "0.16894985155235465\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=200,\n",
       "                          random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=200,\n",
       "                          random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=200,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg = GradientBoostingRegressor(**{'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': 4, 'min_samples_split': 2, 'n_estimators': 200, 'random_state': 42})\n",
    "rfreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df_eval, _, _, _= final_preprocessing(df_eval, std_scaler, dev_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Id     Predicted\n",
      "count   7917.000000   7917.000000\n",
      "mean   35679.634584   1959.751342\n",
      "std     2289.051312   1081.218606\n",
      "min    31715.000000    656.419685\n",
      "25%    33699.000000   1335.983877\n",
      "50%    35680.000000   1705.953098\n",
      "75%    37661.000000   2217.477704\n",
      "max    39643.000000  15057.600037\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfreg.predict(working_df_eval)\n",
    "final_preds = np.exp(y_pred)\n",
    "# Write CSV\n",
    "id_col = df_eval['id']\n",
    "new_df = pd.DataFrame(columns=['Id', 'Predicted'])\n",
    "new_df['Id'] = id_col\n",
    "new_df['Predicted'] = final_preds\n",
    "print(new_df.describe())\n",
    "new_df.to_csv('../output/gboost_with_rfecv.csv', columns=['Id','Predicted'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
