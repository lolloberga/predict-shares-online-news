{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv('../dataset/development.csv')\n",
    "df_eval = pd.read_csv('../dataset/evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcDrop(res):\n",
    "    # All variables with correlation > cutoff\n",
    "    all_corr_vars = list(set(res['v1'].tolist() + res['v2'].tolist()))\n",
    "\n",
    "    # All unique variables in drop column\n",
    "    poss_drop = list(set(res['drop'].tolist()))\n",
    "\n",
    "    # Keep any variable not in drop column\n",
    "    keep = list(set(all_corr_vars).difference(set(poss_drop)))\n",
    "\n",
    "    # Drop any variables in same row as a keep variable\n",
    "    p = res[ res['v1'].isin(keep)  | res['v2'].isin(keep) ][['v1', 'v2']]\n",
    "    q = list(set(p['v1'].tolist() + p['v2'].tolist()))\n",
    "    drop = (list(set(q).difference(set(keep))))\n",
    "\n",
    "    # Remove drop variables from possible drop\n",
    "    poss_drop = list(set(poss_drop).difference(set(drop)))\n",
    "\n",
    "    # subset res dataframe to include possible drop pairs\n",
    "    m = res[ res['v1'].isin(poss_drop)  | res['v2'].isin(poss_drop) ][['v1', 'v2','drop']]\n",
    "\n",
    "    # remove rows that are decided (drop), take set and add to drops\n",
    "    more_drop = set(list(m[~m['v1'].isin(drop) & ~m['v2'].isin(drop)]['drop']))\n",
    "    for item in more_drop:\n",
    "        drop.append(item)\n",
    "\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrX_new(df, cut = 0.9):\n",
    "    # Get correlation matrix and upper triagle\n",
    "    corr_mtx = df.corr().abs()\n",
    "    avg_corr = corr_mtx.mean(axis = 1)\n",
    "    up = corr_mtx.where(np.triu(np.ones(corr_mtx.shape), k=1).astype(np.bool_))\n",
    "    dropcols = list()\n",
    "\n",
    "    res = pd.DataFrame(columns=(['v1', 'v2', 'v1.target',\n",
    "                                 'v2.target','corr', 'drop' ]))\n",
    "    for row in range(len(up)-1):\n",
    "        col_idx = row + 1\n",
    "        for col in range (col_idx, len(up)):\n",
    "            if corr_mtx.iloc[row, col] > cut:\n",
    "                if avg_corr.iloc[row] > avg_corr.iloc[col]:\n",
    "                    dropcols.append(row)\n",
    "                    drop = corr_mtx.columns[row]\n",
    "                else:\n",
    "                    dropcols.append(col)\n",
    "                    drop = corr_mtx.columns[col]\n",
    "\n",
    "                s = pd.Series([ corr_mtx.index[row],\n",
    "                                up.columns[col],\n",
    "                                avg_corr[row],\n",
    "                                avg_corr[col],\n",
    "                                up.iloc[row,col],\n",
    "                                drop],\n",
    "                              index = res.columns)\n",
    "\n",
    "                res.loc[len(res)] = s.to_numpy()\n",
    "\n",
    "    dropcols_names = calcDrop(res)\n",
    "\n",
    "    return dropcols_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_86956/4151814631.py:3: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  corr_mtx = df.corr().abs()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rate_positive_words', 'self_reference_avg_sharess', 'kw_max_max', 'avg_positive_polarity', 'kw_avg_min', 'rate_negative_words', 'abs_title_sentiment_polarity', 'avg_negative_polarity', 'kw_avg_avg', 'n_non_stop_unique_tokens', 'n_non_stop_words']\n"
     ]
    }
   ],
   "source": [
    "drop_new = corrX_new(df_dev, cut = 0.70)\n",
    "print(drop_new)\n",
    "# working_df_dev.drop(drop_new, axis=1, inplace=True)\n",
    "# df_working_df_eval.drop(drop_new, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_preprocessing(df, scaler = None, dev_stats=None):\n",
    "\n",
    "    # one hot encoding\n",
    "    working_df = df.copy()\n",
    "    enc = OneHotEncoder()\n",
    "    encoded_df = pd.concat([working_df['weekday'], working_df['data_channel']], axis=1)\n",
    "    enc.fit(encoded_df)\n",
    "    encoded_df = enc.transform(encoded_df)\n",
    "    additional_columns = enc.get_feature_names_out()\n",
    "    working_df[additional_columns] = encoded_df.toarray()\n",
    "    # print(working_df.shape)\n",
    "\n",
    "    # feature selection from correlation analysis\n",
    "    working_df.drop(['weekday', 'data_channel', 'url', 'id', 'n_tokens_content', 'n_non_stop_words', 'kw_max_min',\n",
    "                'kw_min_max', 'kw_min_min', 'kw_max_avg', 'title_subjectivity', 'rate_positive_words'], axis = 1, inplace=True)\n",
    "    # , 'kw_avg_min', 'kw_avg_avg'\n",
    "    # 'url', 'id','weekday','data_channel'\n",
    "    working_df.drop(['self_reference_avg_sharess', 'kw_max_max', 'avg_positive_polarity', 'rate_negative_words', 'abs_title_sentiment_polarity', 'avg_negative_polarity', 'n_non_stop_unique_tokens'], inplace=True, axis=1)\n",
    "    \n",
    "    # fill missing values\n",
    "    working_df['num_keywords'].fillna(0, inplace=True)\n",
    "    working_df['num_imgs'].fillna(0, inplace=True)\n",
    "    working_df['num_self_hrefs'].fillna(0, inplace=True)\n",
    "    working_df['num_videos'].fillna(0, inplace=True)\n",
    "    if dev_stats == None:\n",
    "        dev_stats = dict()\n",
    "        kw_avg_min_mean =  working_df['kw_avg_min'][working_df['kw_avg_min']>0].mean()\n",
    "        kw_min_avg_mean =  working_df['kw_min_avg'][working_df['kw_min_avg']>0].mean()\n",
    "        working_df['kw_avg_min'] = working_df['kw_avg_min'].apply(lambda x: kw_avg_min_mean if x == -1 else x)\n",
    "        working_df['kw_min_avg'] = working_df['kw_min_avg'].apply(lambda x: kw_min_avg_mean if x == -1 else x)\n",
    "        dev_stats['kw_avg_min_mean'] = kw_avg_min_mean\n",
    "        dev_stats['kw_min_avg_mean'] = kw_min_avg_mean\n",
    "    else:\n",
    "        working_df['kw_avg_min'] = working_df['kw_avg_min'].apply(lambda x: dev_stats['kw_avg_min_mean'] if x == -1 else x)\n",
    "        working_df['kw_min_avg'] = working_df['kw_min_avg'].apply(lambda x: dev_stats['kw_avg_min_mean'] if x == -1 else x)\n",
    "\n",
    "    \n",
    "    if scaler == None:\n",
    "        q1 = working_df['num_hrefs'].describe()['25%']\n",
    "        q3 = working_df['num_hrefs'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_num_hrefs = q1 - 1.5*iqr\n",
    "        max_num_hrefs = q3 + 1.5*iqr\n",
    "        # print(min_num_hrefs, max_num_hrefs)\n",
    "        working_df = working_df[(df.num_hrefs < max_num_hrefs) & (df.num_hrefs > min_num_hrefs)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['kw_avg_min'].describe()['25%']\n",
    "        q3 = working_df['kw_avg_min'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_kw_avg_min = q1 - 1.5*iqr\n",
    "        max_kw_avg_min = q3 + 1.5*iqr\n",
    "        # print(min_kw_avg_min, max_kw_avg_min)\n",
    "        working_df = working_df[(df.kw_avg_min < max_kw_avg_min) & (df.kw_avg_min > min_kw_avg_min)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['num_self_hrefs'].describe()['25%']\n",
    "        q3 = working_df['num_self_hrefs'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_num_self_hrefs = q1 - 1.5*iqr\n",
    "        max_num_self_hrefs = q3 + 1.5*iqr\n",
    "        # print(min_num_self_hrefs, max_num_self_hrefs)\n",
    "        working_df = working_df[(df.num_self_hrefs < max_num_self_hrefs) & (df.num_self_hrefs > min_num_self_hrefs)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['num_imgs'].describe()['25%']\n",
    "        q3 = working_df['num_imgs'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_num_imgs = q1 - 1.5*iqr\n",
    "        max_num_imgs = q3 + 1.5*iqr\n",
    "        # print(min_num_imgs, max_num_imgs)\n",
    "        working_df = working_df[(df.num_imgs < max_num_imgs) & (df.num_imgs > min_num_imgs)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['num_videos'].describe()['25%']\n",
    "        q3 = working_df['num_videos'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_num_videos = q1 - 1.5*iqr\n",
    "        max_num_videos = q3 + 1.5*iqr\n",
    "        # print(min_num_videos, max_num_videos)\n",
    "        working_df = working_df[(df.num_videos < max_num_videos) & (df.num_videos > min_num_videos)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['kw_avg_avg'].describe()['25%']\n",
    "        q3 = working_df['kw_avg_avg'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_kw_avg_avg = q1 - 1.5*iqr\n",
    "        max_kw_avg_avg = q3 + 1.5*iqr\n",
    "        # print(min_kw_avg_avg, max_kw_avg_avg)\n",
    "        working_df = working_df[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "    # log scales\n",
    "    working_df['num_imgs'] = np.log(1 + working_df['num_imgs'])\n",
    "    working_df['num_self_hrefs'] = np.log(1 + working_df['num_self_hrefs'])\n",
    "    working_df['kw_avg_avg'] = np.log(1+working_df['kw_avg_avg'])\n",
    "    working_df['kw_avg_min'] =np.log(1+working_df['kw_avg_min'])\n",
    "        # avoid log on num_videos because it makes it worse\n",
    "\n",
    "    y_dev = None\n",
    "    \n",
    "    # standard scaler\n",
    "    if scaler == None:\n",
    "        print(working_df.shape)\n",
    "        # Remove outliers from kw_avg_avg (we lost another 9% of the dataset)\n",
    "        # q1 = working_df['kw_avg_avg'].describe()['25%']\n",
    "        # q3 = working_df['kw_avg_avg'].describe()['75%']\n",
    "        # iqr = q3 - q1\n",
    "        # min_kw_avg_avg = q1 - 1.5*iqr\n",
    "        # max_kw_avg_avg = q3 + 1.5*iqr\n",
    "        # working_df = working_df[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n",
    "\n",
    "        working_df['shares'] = np.log(working_df['shares'])\n",
    "        y_dev = working_df['shares']\n",
    "        working_df.drop(columns=['shares'], inplace=True)\n",
    "        scaler = StandardScaler().fit(working_df)\n",
    "        scaled_features = scaler.transform(working_df)\n",
    "        working_df[:] = scaled_features[:]\n",
    "\n",
    "        trans = RFECV(estimator=XGBRegressor(), step=1, cv=4 ,n_jobs=-1, verbose=2, scoring='neg_root_mean_squared_error')\n",
    "        trans.fit(working_df, y_dev)\n",
    "        dev_stats['trans'] = trans\n",
    "        working_df = trans.transform(working_df)\n",
    "        \n",
    "    else:\n",
    "        scaled_features = scaler.transform(working_df)\n",
    "        working_df[:] = scaled_features[:]\n",
    "        trans = dev_stats['trans']\n",
    "        working_df = trans.transform(working_df)\n",
    "\n",
    "    # print(scaled_features.shape)\n",
    "    \n",
    "   \n",
    "\n",
    "    return working_df, scaler, y_dev, dev_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_86956/1554209728.py:54: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.kw_avg_min < max_kw_avg_min) & (df.kw_avg_min > min_kw_avg_min)]\n",
      "/var/tmp/ipykernel_86956/1554209728.py:63: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.num_self_hrefs < max_num_self_hrefs) & (df.num_self_hrefs > min_num_self_hrefs)]\n",
      "/var/tmp/ipykernel_86956/1554209728.py:72: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.num_imgs < max_num_imgs) & (df.num_imgs > min_num_imgs)]\n",
      "/var/tmp/ipykernel_86956/1554209728.py:81: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.num_videos < max_num_videos) & (df.num_videos > min_num_videos)]\n",
      "/var/tmp/ipykernel_86956/1554209728.py:90: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n"
     ]
    }
   ],
   "source": [
    "working_df_dev, std_scaler, y_dev, dev_stats = final_preprocessing(df_dev)\n",
    "working_df_eval, _, _, _ = final_preprocessing(df_eval, std_scaler, dev_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = working_df_dev\n",
    "y = y_dev\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7902571669685947\n",
      "0.15969339785764247\n"
     ]
    }
   ],
   "source": [
    "rfreg = GradientBoostingRegressor(**{'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': 4, 'min_samples_split': 2, 'n_estimators': 200, 'random_state': 42})\n",
    "rfreg.fit(X_train, y_train)\n",
    "\n",
    "rms = mean_squared_error(y_valid, rfreg.predict(X_valid), squared=False)\n",
    "print(rms)\n",
    "r2 = r2_score(y_valid, rfreg.predict(X_valid))\n",
    "adj_r2 = 1-(1-r2)*(len(X_valid) - 1)/(len(X_valid) - X_valid.shape[1] - 1)\n",
    "print(adj_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=200,\n",
       "                          random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=200,\n",
       "                          random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=200,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg = GradientBoostingRegressor(**{'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': 4, 'min_samples_split': 2, 'n_estimators': 200, 'random_state': 42})\n",
    "rfreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Id     Predicted\n",
      "count   7917.000000   7917.000000\n",
      "mean   35679.634584   2034.727555\n",
      "std     2289.051312   1114.186086\n",
      "min    31715.000000    455.196388\n",
      "25%    33699.000000   1353.546691\n",
      "50%    35680.000000   1741.818562\n",
      "75%    37661.000000   2350.443842\n",
      "max    39643.000000  18090.205014\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfreg.predict(working_df_eval)\n",
    "final_preds = np.exp(y_pred)\n",
    "# Write CSV\n",
    "id_col = df_eval['id']\n",
    "new_df = pd.DataFrame(columns=['Id', 'Predicted'])\n",
    "new_df['Id'] = id_col\n",
    "new_df['Predicted'] = final_preds\n",
    "print(new_df.describe())\n",
    "new_df.to_csv('../output/rfreg_with_rfecv.csv', columns=['Id','Predicted'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
