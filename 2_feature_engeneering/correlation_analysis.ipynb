{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T21:16:09.665900Z",
     "start_time": "2023-06-13T21:16:09.657726Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:49:09.058252Z",
     "start_time": "2023-06-13T22:49:08.408251Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://mashable.com/2014/09/08/safest-cabbies-...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.545031</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160714</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>bus</td>\n",
       "      <td>tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://mashable.com/2013/07/25/3d-printed-rifle/</td>\n",
       "      <td>532.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>0.569697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.737542</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157500</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>tech</td>\n",
       "      <td>thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://mashable.com/2013/10/30/digital-dinosau...</td>\n",
       "      <td>435.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.646018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.748428</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.427500</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17700.0</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://mashable.com/2014/08/27/homer-simpson-i...</td>\n",
       "      <td>134.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>bus</td>\n",
       "      <td>wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://mashable.com/2013/01/10/creepy-robotic-...</td>\n",
       "      <td>728.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.652632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251786</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>tech</td>\n",
       "      <td>thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                url  timedelta  \\\n",
       "0   0  http://mashable.com/2014/09/08/safest-cabbies-...      121.0   \n",
       "1   1   http://mashable.com/2013/07/25/3d-printed-rifle/      532.0   \n",
       "2   2  http://mashable.com/2013/10/30/digital-dinosau...      435.0   \n",
       "3   3  http://mashable.com/2014/08/27/homer-simpson-i...      134.0   \n",
       "4   4  http://mashable.com/2013/01/10/creepy-robotic-...      728.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0            12.0            1015.0         0.422018               1.0   \n",
       "1             9.0             503.0         0.569697               1.0   \n",
       "2             9.0             232.0         0.646018               1.0   \n",
       "3            12.0             171.0         0.722892               1.0   \n",
       "4            11.0             286.0         0.652632               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  ...  \\\n",
       "0                  0.545031       10.0             6.0  ...   \n",
       "1                  0.737542        9.0             0.0  ...   \n",
       "2                  0.748428       12.0             3.0  ...   \n",
       "3                  0.867925        9.0             5.0  ...   \n",
       "4                  0.800000        5.0             2.0  ...   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.160714                  -0.50              -0.071429   \n",
       "1              -0.157500                  -0.25              -0.100000   \n",
       "2              -0.427500                  -1.00              -0.187500   \n",
       "3              -0.216667                  -0.25              -0.166667   \n",
       "4              -0.251786                  -0.50              -0.100000   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                 0.0                      0.00                     0.5   \n",
       "1                 0.0                      0.00                     0.5   \n",
       "2                 0.0                      0.00                     0.5   \n",
       "3                 0.4                     -0.25                     0.1   \n",
       "4                 0.2                     -0.10                     0.3   \n",
       "\n",
       "   abs_title_sentiment_polarity   shares  data_channel    weekday  \n",
       "0                          0.00   2900.0           bus    tuesday  \n",
       "1                          0.00   1300.0          tech   thursday  \n",
       "2                          0.00  17700.0     lifestyle  wednesday  \n",
       "3                          0.25   1500.0           bus  wednesday  \n",
       "4                          0.10   1400.0          tech   thursday  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = pd.read_csv('../dataset/development.csv')\n",
    "df_eval = pd.read_csv('../dataset/evaluation.csv')\n",
    "\n",
    "df = pd.concat([df_dev, df_eval], sort=False)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:53:31.174528Z",
     "start_time": "2023-06-13T22:53:31.157947Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_preprocessing(df, reduce_df=True):\n",
    "    working_df_dev = df.copy()\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    encoded_df = pd.concat([df['weekday'], df['data_channel']], axis=1)\n",
    "    enc.fit(encoded_df)\n",
    "    encoded_df = enc.transform(encoded_df)\n",
    "    additional_columns = enc.get_feature_names_out()\n",
    "    working_df_dev[additional_columns] = encoded_df.toarray()\n",
    "    working_df_dev.drop(['weekday', 'data_channel', 'url', 'id'], axis = 1, inplace=True)\n",
    "\n",
    "    working_df_dev['num_keywords'] = df.groupby(['data_channel'], sort=False)['num_keywords'].apply(lambda x: x.fillna(x.mean())).reset_index()['num_keywords']\n",
    "\n",
    "    working_df_dev['n_tokens_content'] = np.log(1 + working_df_dev['n_tokens_content'])\n",
    "\n",
    "    if 'shares' in working_df_dev:\n",
    "        working_df_dev['shares'] = np.log(working_df_dev['shares'])\n",
    "\n",
    "    if reduce_df:\n",
    "        # Remove outliers from kw_avg_avg (we lost another 9% of the dataset)\n",
    "        q1 = working_df_dev['kw_avg_avg'].describe()['25%']\n",
    "        q3 = working_df_dev['kw_avg_avg'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_kw_avg_avg = q1 - 1.5*iqr\n",
    "        max_kw_avg_avg = q3 + 1.5*iqr\n",
    "        working_df_dev = working_df_dev[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n",
    "\n",
    "    std_scaler = StandardScaler().fit(working_df_dev[['kw_avg_max', 'kw_avg_avg', 'kw_avg_min', 'kw_min_avg', 'kw_max_avg', 'kw_max_min', 'kw_min_max']])\n",
    "    scaled_features = std_scaler.transform(working_df_dev[['kw_avg_max', 'kw_avg_avg', 'kw_avg_min', 'kw_min_avg','kw_max_avg', 'kw_max_min', 'kw_min_max']])\n",
    "    working_df_dev[['kw_avg_max', 'kw_avg_avg', 'kw_avg_min', 'kw_min_avg','kw_max_avg', 'kw_max_min', 'kw_min_max']] = scaled_features\n",
    "\n",
    "    std_scaler = StandardScaler().fit(working_df_dev[['self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess']])\n",
    "    scaled_features = std_scaler.transform(working_df_dev[['self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess']])\n",
    "    working_df_dev[['self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess']] = scaled_features\n",
    "\n",
    "    std_scaler = StandardScaler().fit(working_df_dev[['n_tokens_title', 'n_tokens_content']])\n",
    "    scaled_features = std_scaler.transform(working_df_dev[['n_tokens_title', 'n_tokens_content']])\n",
    "    working_df_dev[['n_tokens_title', 'n_tokens_content']] = scaled_features\n",
    "\n",
    "    working_df_dev['num_imgs'].fillna(working_df_dev['num_imgs'].mean(), inplace=True)\n",
    "    working_df_dev['num_imgs'] = np.log(1 + working_df_dev['num_imgs'])\n",
    "\n",
    "    working_df_dev['num_self_hrefs'].fillna(working_df_dev['num_self_hrefs'].mean(), inplace=True)\n",
    "    working_df_dev['num_self_hrefs'] = np.log(1 + working_df_dev['num_self_hrefs'])\n",
    "\n",
    "    working_df_dev['num_videos'].fillna(working_df_dev['num_videos'].mean(), inplace=True)\n",
    "    working_df_dev['num_videos'] = np.log(1 + working_df_dev['num_videos'])\n",
    "\n",
    "    is_weekend = []\n",
    "    for _, row in working_df_dev.iterrows():\n",
    "        if row['weekday_sunday'] == 1 or row['weekday_saturday'] == 1:\n",
    "            is_weekend.append(1)\n",
    "        else:\n",
    "            is_weekend.append(0)\n",
    "    working_df_dev['is_weekend'] = is_weekend\n",
    "    working_df_dev.drop(columns=[x for x in additional_columns if x.startswith('weekday')], inplace=True)\n",
    "\n",
    "\n",
    "    std_scaler = StandardScaler().fit(working_df_dev[['timedelta']])\n",
    "    scaled_features = std_scaler.transform(working_df_dev[['timedelta']])\n",
    "    working_df_dev[['timedelta']] = scaled_features\n",
    "\n",
    "    return working_df_dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:49:15.400557Z",
     "start_time": "2023-06-13T22:49:15.389192Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corrX_orig(df, cut = 0.9) :\n",
    "    # Get correlation matrix and upper triagle\n",
    "    corr_mtx = df.corr().abs()\n",
    "    avg_corr = corr_mtx.mean(axis = 1)\n",
    "    up = corr_mtx.where(np.triu(np.ones(corr_mtx.shape), k=1).astype(np.bool_))\n",
    "\n",
    "    drop = list()\n",
    "\n",
    "    # For loop implements this pseudocode\n",
    "    # For every cell in the upper triangle:\n",
    "    # If cell.value > 0.6:\n",
    "    # If mean(row_correlation) > mean(column_correlation):drop(column)\n",
    "    # Else: drop(row)\n",
    "    for row in range(len(up)-1):\n",
    "        col_idx = row + 1\n",
    "        for col in range (col_idx, len(up)):\n",
    "            if corr_mtx.iloc[row, col] > cut:\n",
    "                if avg_corr.iloc[row] > avg_corr.iloc[col]:\n",
    "                    drop.append(row)\n",
    "                else:\n",
    "                    drop.append(col)\n",
    "\n",
    "    drop_set = list(set(drop))\n",
    "    # dropcols_idx = drop_set\n",
    "    dropcols_names = list(df.columns[[item for item in drop_set]])\n",
    "\n",
    "    return dropcols_names\n",
    "\n",
    "def calcDrop(res):\n",
    "    # All variables with correlation > cutoff\n",
    "    all_corr_vars = list(set(res['v1'].tolist() + res['v2'].tolist()))\n",
    "\n",
    "    # All unique variables in drop column\n",
    "    poss_drop = list(set(res['drop'].tolist()))\n",
    "\n",
    "    # Keep any variable not in drop column\n",
    "    keep = list(set(all_corr_vars).difference(set(poss_drop)))\n",
    "\n",
    "    # Drop any variables in same row as a keep variable\n",
    "    p = res[ res['v1'].isin(keep)  | res['v2'].isin(keep) ][['v1', 'v2']]\n",
    "    q = list(set(p['v1'].tolist() + p['v2'].tolist()))\n",
    "    drop = (list(set(q).difference(set(keep))))\n",
    "\n",
    "    # Remove drop variables from possible drop\n",
    "    poss_drop = list(set(poss_drop).difference(set(drop)))\n",
    "\n",
    "    # subset res dataframe to include possible drop pairs\n",
    "    m = res[ res['v1'].isin(poss_drop)  | res['v2'].isin(poss_drop) ][['v1', 'v2','drop']]\n",
    "\n",
    "    # remove rows that are decided (drop), take set and add to drops\n",
    "    more_drop = set(list(m[~m['v1'].isin(drop) & ~m['v2'].isin(drop)]['drop']))\n",
    "    for item in more_drop:\n",
    "        drop.append(item)\n",
    "\n",
    "    return drop\n",
    "\n",
    "def corrX_new(df, cut = 0.9):\n",
    "    # Get correlation matrix and upper triagle\n",
    "    corr_mtx = df.corr().abs()\n",
    "    avg_corr = corr_mtx.mean(axis = 1)\n",
    "    up = corr_mtx.where(np.triu(np.ones(corr_mtx.shape), k=1).astype(np.bool_))\n",
    "    dropcols = list()\n",
    "\n",
    "    res = pd.DataFrame(columns=(['v1', 'v2', 'v1.target',\n",
    "                                 'v2.target','corr', 'drop' ]))\n",
    "    for row in range(len(up)-1):\n",
    "        col_idx = row + 1\n",
    "        for col in range (col_idx, len(up)):\n",
    "            if corr_mtx.iloc[row, col] > cut:\n",
    "                if avg_corr.iloc[row] > avg_corr.iloc[col]:\n",
    "                    dropcols.append(row)\n",
    "                    drop = corr_mtx.columns[row]\n",
    "                else:\n",
    "                    dropcols.append(col)\n",
    "                    drop = corr_mtx.columns[col]\n",
    "\n",
    "                s = pd.Series([ corr_mtx.index[row],\n",
    "                                up.columns[col],\n",
    "                                avg_corr[row],\n",
    "                                avg_corr[col],\n",
    "                                up.iloc[row,col],\n",
    "                                drop],\n",
    "                              index = res.columns)\n",
    "\n",
    "                res.loc[len(res)] = s.to_numpy()\n",
    "\n",
    "    dropcols_names = calcDrop(res)\n",
    "\n",
    "    return dropcols_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:50:36.278209Z",
     "start_time": "2023-06-13T22:50:34.494745Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df_dev = final_preprocessing(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:50:40.176107Z",
     "start_time": "2023-06-13T22:50:39.930935Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop = corrX_orig(working_df_dev, cut = 0.7)\n",
    "len(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:50:42.486283Z",
     "start_time": "2023-06-13T22:50:41.963690Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "drop_new = corrX_new(working_df_dev, cut = 0.7)\n",
    "print(len(drop_new))\n",
    "print(list(set(drop).difference(set(drop_new))))\n",
    "drop_df = working_df_dev.drop(drop_new, axis=1)\n",
    "\n",
    "# check the algorithm:\n",
    "drop_origin = corrX_orig(drop_df, cut = 0.7)\n",
    "print(len(drop_origin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:50:51.520059Z",
     "start_time": "2023-06-13T22:50:51.510081Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30410, 39)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39,)\n",
      "abs_title_subjectivity        0.000774\n",
      "max_negative_polarity         0.002287\n",
      "data_channel_bus              0.012113\n",
      "min_negative_polarity         0.012955\n",
      "kw_min_min                    0.016170\n",
      "n_tokens_content              0.018708\n",
      "kw_avg_max                    0.020984\n",
      "global_rate_negative_words    0.024605\n",
      "num_videos                    0.033386\n",
      "title_subjectivity            0.033993\n",
      "n_tokens_title                0.041417\n",
      "kw_min_max                    0.041771\n",
      "num_self_hrefs                0.041915\n",
      "LDA_03                        0.043249\n",
      "timedelta                     0.044614\n",
      "avg_positive_polarity         0.048181\n",
      "n_unique_tokens               0.050354\n",
      "min_positive_polarity         0.050888\n",
      "max_positive_polarity         0.057181\n",
      "title_sentiment_polarity      0.059690\n",
      "num_imgs                      0.073682\n",
      "num_keywords                  0.073807\n",
      "LDA_01                        0.076072\n",
      "global_rate_positive_words    0.079688\n",
      "kw_max_min                    0.085748\n",
      "global_sentiment_polarity     0.086386\n",
      "num_hrefs                     0.086639\n",
      "kw_min_avg                    0.086892\n",
      "data_channel_tech             0.107956\n",
      "global_subjectivity           0.110067\n",
      "data_channel_entertainment    0.116879\n",
      "data_channel_lifestyle        0.117508\n",
      "data_channel_socmed           0.119805\n",
      "LDA_02                        0.149440\n",
      "is_weekend                    0.154787\n",
      "self_reference_max_shares     0.165962\n",
      "self_reference_min_shares     0.174314\n",
      "kw_max_avg                    0.211706\n",
      "Name: shares, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'num_keywords',\n",
       "       'kw_min_min', 'kw_max_min', 'kw_min_max', 'kw_avg_max', 'kw_min_avg',\n",
       "       'kw_max_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
       "       'LDA_01', 'LDA_02', 'LDA_03', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'avg_positive_polarity',\n",
       "       'min_positive_polarity', 'max_positive_polarity',\n",
       "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity', 'shares',\n",
       "       'data_channel_bus', 'data_channel_entertainment',\n",
       "       'data_channel_lifestyle', 'data_channel_socmed', 'data_channel_tech',\n",
       "       'is_weekend'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mat_shares = drop_df.corr(method='spearman')['shares']\n",
    "print(corr_mat_shares.shape)\n",
    "corr_mat_shares.drop(['shares'], inplace=True)\n",
    "print(corr_mat_shares.abs().sort_values())\n",
    "corr_mat_shares.drop(['max_negative_polarity', 'abs_title_subjectivity', 'data_channel_bus'], inplace = True)\n",
    "drop_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Test some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:50:53.047780Z",
     "start_time": "2023-06-13T22:50:53.012972Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = drop_df.drop(columns=[\"shares\"]).values\n",
    "y = drop_df[\"shares\"].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:51:33.883209Z",
     "start_time": "2023-06-13T22:50:53.580942Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8412815728456365\n",
      "0.12984042201043433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nResults with no final_preprocessing function:\\n0.8403925418687583\\n0.13167854634661713\\nResults with final_preprocessing function:\\n0.8404129967812869\\n0.13163627644380282\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = RandomForestRegressor(100, max_depth=50, min_samples_split=10, random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "rms = mean_squared_error(y_valid, reg.predict(X_valid), squared=False)\n",
    "print(rms)\n",
    "r2 = r2_score(y_valid, reg.predict(X_valid))\n",
    "adj_r2 = 1 - (1 - r2) * (len(X_valid) - 1) / (len(X_valid) - X_valid.shape[1] - 1)\n",
    "print(adj_r2)\n",
    "'''\n",
    "Results with no final_preprocessing function:\n",
    "0.8403925418687583\n",
    "0.13167854634661713\n",
    "Results with final_preprocessing function:\n",
    "0.8404129967812869\n",
    "0.13163627644380282\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:53:40.786427Z",
     "start_time": "2023-06-13T22:53:40.256238Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7917 7917\n",
      "(7917, 38) (7917, 52)\n"
     ]
    }
   ],
   "source": [
    "df_working_df_eval = final_preprocessing(df_eval, reduce_df=False)\n",
    "print(len(df_working_df_eval), len(df_eval))\n",
    "\n",
    "drop_df_eval = df_working_df_eval.drop(drop_new, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:54:19.639200Z",
     "start_time": "2023-06-13T22:54:19.618884Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30410, 38)\n"
     ]
    }
   ],
   "source": [
    "X = drop_df.drop(columns=[\"shares\"]).values\n",
    "y = drop_df[\"shares\"].values\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:55:28.431923Z",
     "start_time": "2023-06-13T22:54:31.692522Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Id    Predicted\n",
      "count   7917.000000  7917.000000\n",
      "mean   35679.634584  1988.046783\n",
      "std     2289.051312   707.967744\n",
      "min    31715.000000   528.201120\n",
      "25%    33699.000000  1475.462859\n",
      "50%    35680.000000  1856.176614\n",
      "75%    37661.000000  2345.768323\n",
      "max    39643.000000  8564.067723\n"
     ]
    }
   ],
   "source": [
    "best_model = RandomForestRegressor(100, max_depth=50, min_samples_split=10, random_state=42)\n",
    "best_model.fit(X, y)\n",
    "# Make final predictions\n",
    "y_pred = best_model.predict(drop_df_eval.values)\n",
    "final_preds = np.exp(y_pred)\n",
    "# Write CSV\n",
    "id_col = df_eval['id']\n",
    "new_df = pd.DataFrame(columns=['Id', 'Predicted'])\n",
    "new_df['Id'] = id_col\n",
    "new_df['Predicted'] = final_preds\n",
    "print(new_df.describe())\n",
    "new_df.to_csv('../output/correlation_analysis_and_random_regress.csv', columns=['Id', 'Predicted'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
