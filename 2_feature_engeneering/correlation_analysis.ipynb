{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-13T21:16:09.665900Z",
     "start_time": "2023-06-13T21:16:09.657726Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "data": {
      "text/plain": "   id                                                url  timedelta  \\\n0   0  http://mashable.com/2014/09/08/safest-cabbies-...      121.0   \n1   1   http://mashable.com/2013/07/25/3d-printed-rifle/      532.0   \n2   2  http://mashable.com/2013/10/30/digital-dinosau...      435.0   \n3   3  http://mashable.com/2014/08/27/homer-simpson-i...      134.0   \n4   4  http://mashable.com/2013/01/10/creepy-robotic-...      728.0   \n\n   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n0            12.0            1015.0         0.422018               1.0   \n1             9.0             503.0         0.569697               1.0   \n2             9.0             232.0         0.646018               1.0   \n3            12.0             171.0         0.722892               1.0   \n4            11.0             286.0         0.652632               1.0   \n\n   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  ...  \\\n0                  0.545031       10.0             6.0  ...   \n1                  0.737542        9.0             0.0  ...   \n2                  0.748428       12.0             3.0  ...   \n3                  0.867925        9.0             5.0  ...   \n4                  0.800000        5.0             2.0  ...   \n\n   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n0              -0.160714                  -0.50              -0.071429   \n1              -0.157500                  -0.25              -0.100000   \n2              -0.427500                  -1.00              -0.187500   \n3              -0.216667                  -0.25              -0.166667   \n4              -0.251786                  -0.50              -0.100000   \n\n   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n0                 0.0                      0.00                     0.5   \n1                 0.0                      0.00                     0.5   \n2                 0.0                      0.00                     0.5   \n3                 0.4                     -0.25                     0.1   \n4                 0.2                     -0.10                     0.3   \n\n   abs_title_sentiment_polarity   shares  data_channel    weekday  \n0                          0.00   2900.0           bus    tuesday  \n1                          0.00   1300.0          tech   thursday  \n2                          0.00  17700.0     lifestyle  wednesday  \n3                          0.25   1500.0           bus  wednesday  \n4                          0.10   1400.0          tech   thursday  \n\n[5 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url</th>\n      <th>timedelta</th>\n      <th>n_tokens_title</th>\n      <th>n_tokens_content</th>\n      <th>n_unique_tokens</th>\n      <th>n_non_stop_words</th>\n      <th>n_non_stop_unique_tokens</th>\n      <th>num_hrefs</th>\n      <th>num_self_hrefs</th>\n      <th>...</th>\n      <th>avg_negative_polarity</th>\n      <th>min_negative_polarity</th>\n      <th>max_negative_polarity</th>\n      <th>title_subjectivity</th>\n      <th>title_sentiment_polarity</th>\n      <th>abs_title_subjectivity</th>\n      <th>abs_title_sentiment_polarity</th>\n      <th>shares</th>\n      <th>data_channel</th>\n      <th>weekday</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>http://mashable.com/2014/09/08/safest-cabbies-...</td>\n      <td>121.0</td>\n      <td>12.0</td>\n      <td>1015.0</td>\n      <td>0.422018</td>\n      <td>1.0</td>\n      <td>0.545031</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>-0.160714</td>\n      <td>-0.50</td>\n      <td>-0.071429</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>2900.0</td>\n      <td>bus</td>\n      <td>tuesday</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>http://mashable.com/2013/07/25/3d-printed-rifle/</td>\n      <td>532.0</td>\n      <td>9.0</td>\n      <td>503.0</td>\n      <td>0.569697</td>\n      <td>1.0</td>\n      <td>0.737542</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.157500</td>\n      <td>-0.25</td>\n      <td>-0.100000</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>1300.0</td>\n      <td>tech</td>\n      <td>thursday</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>http://mashable.com/2013/10/30/digital-dinosau...</td>\n      <td>435.0</td>\n      <td>9.0</td>\n      <td>232.0</td>\n      <td>0.646018</td>\n      <td>1.0</td>\n      <td>0.748428</td>\n      <td>12.0</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>-0.427500</td>\n      <td>-1.00</td>\n      <td>-0.187500</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>17700.0</td>\n      <td>lifestyle</td>\n      <td>wednesday</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>http://mashable.com/2014/08/27/homer-simpson-i...</td>\n      <td>134.0</td>\n      <td>12.0</td>\n      <td>171.0</td>\n      <td>0.722892</td>\n      <td>1.0</td>\n      <td>0.867925</td>\n      <td>9.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>-0.216667</td>\n      <td>-0.25</td>\n      <td>-0.166667</td>\n      <td>0.4</td>\n      <td>-0.25</td>\n      <td>0.1</td>\n      <td>0.25</td>\n      <td>1500.0</td>\n      <td>bus</td>\n      <td>wednesday</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>http://mashable.com/2013/01/10/creepy-robotic-...</td>\n      <td>728.0</td>\n      <td>11.0</td>\n      <td>286.0</td>\n      <td>0.652632</td>\n      <td>1.0</td>\n      <td>0.800000</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>-0.251786</td>\n      <td>-0.50</td>\n      <td>-0.100000</td>\n      <td>0.2</td>\n      <td>-0.10</td>\n      <td>0.3</td>\n      <td>0.10</td>\n      <td>1400.0</td>\n      <td>tech</td>\n      <td>thursday</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 50 columns</p>\n</div>"
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = pd.read_csv('../dataset/development.csv')\n",
    "df_eval = pd.read_csv('../dataset/evaluation.csv')\n",
    "\n",
    "df = pd.concat([df_dev, df_eval], sort=False)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:49:09.058252Z",
     "start_time": "2023-06-13T22:49:08.408251Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "outputs": [],
   "source": [
    "def final_preprocessing(df, reduce_df=True):\n",
    "    working_df_dev = df.copy()\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    encoded_df = pd.concat([df['weekday'], df['data_channel']], axis=1)\n",
    "    enc.fit(encoded_df)\n",
    "    encoded_df = enc.transform(encoded_df)\n",
    "    additional_columns = enc.get_feature_names_out()\n",
    "    working_df_dev[additional_columns] = encoded_df.toarray()\n",
    "    working_df_dev.drop(['weekday', 'data_channel', 'url', 'id'], axis = 1, inplace=True)\n",
    "\n",
    "    working_df_dev['num_keywords'] = df.groupby(['data_channel'], sort=False)['num_keywords'].apply(lambda x: x.fillna(x.mean())).reset_index()['num_keywords']\n",
    "\n",
    "    working_df_dev['n_tokens_content'] = np.log(1 + working_df_dev['n_tokens_content'])\n",
    "\n",
    "    if 'shares' in working_df_dev:\n",
    "        working_df_dev['shares'] = np.log(working_df_dev['shares'])\n",
    "\n",
    "    if reduce_df:\n",
    "        # Remove outliers from kw_avg_avg (we lost another 9% of the dataset)\n",
    "        q1 = working_df_dev['kw_avg_avg'].describe()['25%']\n",
    "        q3 = working_df_dev['kw_avg_avg'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_kw_avg_avg = q1 - 1.5*iqr\n",
    "        max_kw_avg_avg = q3 + 1.5*iqr\n",
    "        working_df_dev = working_df_dev[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n",
    "\n",
    "    std_scaler = StandardScaler().fit(working_df_dev[['kw_avg_max', 'kw_avg_avg', 'kw_avg_min', 'kw_min_avg', 'kw_max_avg', 'kw_max_min', 'kw_min_max']])\n",
    "    scaled_features = std_scaler.transform(working_df_dev[['kw_avg_max', 'kw_avg_avg', 'kw_avg_min', 'kw_min_avg','kw_max_avg', 'kw_max_min', 'kw_min_max']])\n",
    "    working_df_dev[['kw_avg_max', 'kw_avg_avg', 'kw_avg_min', 'kw_min_avg','kw_max_avg', 'kw_max_min', 'kw_min_max']] = scaled_features\n",
    "\n",
    "    std_scaler = StandardScaler().fit(working_df_dev[['self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess']])\n",
    "    scaled_features = std_scaler.transform(working_df_dev[['self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess']])\n",
    "    working_df_dev[['self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess']] = scaled_features\n",
    "\n",
    "    std_scaler = StandardScaler().fit(working_df_dev[['n_tokens_title', 'n_tokens_content']])\n",
    "    scaled_features = std_scaler.transform(working_df_dev[['n_tokens_title', 'n_tokens_content']])\n",
    "    working_df_dev[['n_tokens_title', 'n_tokens_content']] = scaled_features\n",
    "\n",
    "    working_df_dev['num_imgs'].fillna(working_df_dev['num_imgs'].mean(), inplace=True)\n",
    "    working_df_dev['num_imgs'] = np.log(1 + working_df_dev['num_imgs'])\n",
    "\n",
    "    working_df_dev['num_self_hrefs'].fillna(working_df_dev['num_self_hrefs'].mean(), inplace=True)\n",
    "    working_df_dev['num_self_hrefs'] = np.log(1 + working_df_dev['num_self_hrefs'])\n",
    "\n",
    "    working_df_dev['num_videos'].fillna(working_df_dev['num_videos'].mean(), inplace=True)\n",
    "    working_df_dev['num_videos'] = np.log(1 + working_df_dev['num_videos'])\n",
    "\n",
    "    is_weekend = []\n",
    "    for _, row in working_df_dev.iterrows():\n",
    "        if row['weekday_sunday'] == 1 or row['weekday_saturday'] == 1:\n",
    "            is_weekend.append(1)\n",
    "        else:\n",
    "            is_weekend.append(0)\n",
    "    working_df_dev['is_weekend'] = is_weekend\n",
    "    working_df_dev.drop(columns=[x for x in additional_columns if x.startswith('weekday')], inplace=True)\n",
    "\n",
    "\n",
    "    std_scaler = StandardScaler().fit(working_df_dev[['timedelta']])\n",
    "    scaled_features = std_scaler.transform(working_df_dev[['timedelta']])\n",
    "    working_df_dev[['timedelta']] = scaled_features\n",
    "\n",
    "    return working_df_dev"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:53:31.174528Z",
     "start_time": "2023-06-13T22:53:31.157947Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correlation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "def corrX_orig(df, cut = 0.9) :\n",
    "    # Get correlation matrix and upper triagle\n",
    "    corr_mtx = df.corr().abs()\n",
    "    avg_corr = corr_mtx.mean(axis = 1)\n",
    "    up = corr_mtx.where(np.triu(np.ones(corr_mtx.shape), k=1).astype(np.bool_))\n",
    "\n",
    "    drop = list()\n",
    "\n",
    "    # For loop implements this pseudocode\n",
    "    # For every cell in the upper triangle:\n",
    "    # If cell.value > 0.6:\n",
    "    # If mean(row_correlation) > mean(column_correlation):drop(column)\n",
    "    # Else: drop(row)\n",
    "    for row in range(len(up)-1):\n",
    "        col_idx = row + 1\n",
    "        for col in range (col_idx, len(up)):\n",
    "            if corr_mtx.iloc[row, col] > cut:\n",
    "                if avg_corr.iloc[row] > avg_corr.iloc[col]:\n",
    "                    drop.append(row)\n",
    "                else:\n",
    "                    drop.append(col)\n",
    "\n",
    "    drop_set = list(set(drop))\n",
    "    # dropcols_idx = drop_set\n",
    "    dropcols_names = list(df.columns[[item for item in drop_set]])\n",
    "\n",
    "    return dropcols_names\n",
    "\n",
    "def calcDrop(res):\n",
    "    # All variables with correlation > cutoff\n",
    "    all_corr_vars = list(set(res['v1'].tolist() + res['v2'].tolist()))\n",
    "\n",
    "    # All unique variables in drop column\n",
    "    poss_drop = list(set(res['drop'].tolist()))\n",
    "\n",
    "    # Keep any variable not in drop column\n",
    "    keep = list(set(all_corr_vars).difference(set(poss_drop)))\n",
    "\n",
    "    # Drop any variables in same row as a keep variable\n",
    "    p = res[ res['v1'].isin(keep)  | res['v2'].isin(keep) ][['v1', 'v2']]\n",
    "    q = list(set(p['v1'].tolist() + p['v2'].tolist()))\n",
    "    drop = (list(set(q).difference(set(keep))))\n",
    "\n",
    "    # Remove drop variables from possible drop\n",
    "    poss_drop = list(set(poss_drop).difference(set(drop)))\n",
    "\n",
    "    # subset res dataframe to include possible drop pairs\n",
    "    m = res[ res['v1'].isin(poss_drop)  | res['v2'].isin(poss_drop) ][['v1', 'v2','drop']]\n",
    "\n",
    "    # remove rows that are decided (drop), take set and add to drops\n",
    "    more_drop = set(list(m[~m['v1'].isin(drop) & ~m['v2'].isin(drop)]['drop']))\n",
    "    for item in more_drop:\n",
    "        drop.append(item)\n",
    "\n",
    "    return drop\n",
    "\n",
    "def corrX_new(df, cut = 0.9):\n",
    "    # Get correlation matrix and upper triagle\n",
    "    corr_mtx = df.corr().abs()\n",
    "    avg_corr = corr_mtx.mean(axis = 1)\n",
    "    up = corr_mtx.where(np.triu(np.ones(corr_mtx.shape), k=1).astype(np.bool_))\n",
    "    dropcols = list()\n",
    "\n",
    "    res = pd.DataFrame(columns=(['v1', 'v2', 'v1.target',\n",
    "                                 'v2.target','corr', 'drop' ]))\n",
    "    for row in range(len(up)-1):\n",
    "        col_idx = row + 1\n",
    "        for col in range (col_idx, len(up)):\n",
    "            if corr_mtx.iloc[row, col] > cut:\n",
    "                if avg_corr.iloc[row] > avg_corr.iloc[col]:\n",
    "                    dropcols.append(row)\n",
    "                    drop = corr_mtx.columns[row]\n",
    "                else:\n",
    "                    dropcols.append(col)\n",
    "                    drop = corr_mtx.columns[col]\n",
    "\n",
    "                s = pd.Series([ corr_mtx.index[row],\n",
    "                                up.columns[col],\n",
    "                                avg_corr[row],\n",
    "                                avg_corr[col],\n",
    "                                up.iloc[row,col],\n",
    "                                drop],\n",
    "                              index = res.columns)\n",
    "\n",
    "                res.loc[len(res)] = s.to_numpy()\n",
    "\n",
    "    dropcols_names = calcDrop(res)\n",
    "\n",
    "    return dropcols_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:49:15.400557Z",
     "start_time": "2023-06-13T22:49:15.389192Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [],
   "source": [
    "working_df_dev = final_preprocessing(df_dev)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:50:36.278209Z",
     "start_time": "2023-06-13T22:50:34.494745Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [
    {
     "data": {
      "text/plain": "14"
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop = corrX_orig(working_df_dev, cut = 0.7)\n",
    "len(drop)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:50:40.176107Z",
     "start_time": "2023-06-13T22:50:39.930935Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "drop_new = corrX_new(working_df_dev, cut = 0.7)\n",
    "print(len(drop_new))\n",
    "print(list(set(drop).difference(set(drop_new))))\n",
    "drop_df = working_df_dev.drop(drop_new, axis=1)\n",
    "\n",
    "# check the algorithm:\n",
    "drop_origin = corrX_orig(drop_df, cut = 0.7)\n",
    "print(len(drop_origin))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:50:42.486283Z",
     "start_time": "2023-06-13T22:50:41.963690Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [
    {
     "data": {
      "text/plain": "(30410, 39)"
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:50:51.520059Z",
     "start_time": "2023-06-13T22:50:51.510081Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test some models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [],
   "source": [
    "X = drop_df.drop(columns=[\"shares\"]).values\n",
    "y = drop_df[\"shares\"].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle=True, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:50:53.047780Z",
     "start_time": "2023-06-13T22:50:53.012972Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8404129967812869\n",
      "0.13163627644380282\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nResults with no final_preprocessing function:\\n0.8403925418687583\\n0.13167854634661713\\nResults with final_preprocessing function:\\n0.8407777114640858\\n0.1301924655586867\\n'"
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = RandomForestRegressor(100, max_depth=50, min_samples_split=10, random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "rms = mean_squared_error(y_valid, reg.predict(X_valid), squared=False)\n",
    "print(rms)\n",
    "r2 = r2_score(y_valid, reg.predict(X_valid))\n",
    "adj_r2 = 1 - (1 - r2) * (len(X_valid) - 1) / (len(X_valid) - X_valid.shape[1] - 1)\n",
    "print(adj_r2)\n",
    "'''\n",
    "Results with no final_preprocessing function:\n",
    "0.8403925418687583\n",
    "0.13167854634661713\n",
    "Results with final_preprocessing function:\n",
    "0.8407777114640858\n",
    "0.1301924655586867\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:51:33.883209Z",
     "start_time": "2023-06-13T22:50:53.580942Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate final CSV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7917 7917\n",
      "(7917, 38) (7917, 52)\n"
     ]
    }
   ],
   "source": [
    "df_working_df_eval = final_preprocessing(df_eval, reduce_df=False)\n",
    "print(len(df_working_df_eval), len(df_eval))\n",
    "\n",
    "drop_df_eval = df_working_df_eval.drop(drop_new, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:53:40.786427Z",
     "start_time": "2023-06-13T22:53:40.256238Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30410, 38)\n"
     ]
    }
   ],
   "source": [
    "X = drop_df.drop(columns=[\"shares\"]).values\n",
    "y = drop_df[\"shares\"].values\n",
    "print(X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:54:19.639200Z",
     "start_time": "2023-06-13T22:54:19.618884Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Id    Predicted\n",
      "count   7917.000000  7917.000000\n",
      "mean   35679.634584  1988.046783\n",
      "std     2289.051312   707.967744\n",
      "min    31715.000000   528.201120\n",
      "25%    33699.000000  1475.462859\n",
      "50%    35680.000000  1856.176614\n",
      "75%    37661.000000  2345.768323\n",
      "max    39643.000000  8564.067723\n"
     ]
    }
   ],
   "source": [
    "best_model = RandomForestRegressor(100, max_depth=50, min_samples_split=10, random_state=42)\n",
    "best_model.fit(X, y)\n",
    "# Make final predictions\n",
    "y_pred = best_model.predict(drop_df_eval.values)\n",
    "final_preds = np.exp(y_pred)\n",
    "# Write CSV\n",
    "id_col = df_eval['id']\n",
    "new_df = pd.DataFrame(columns=['Id', 'Predicted'])\n",
    "new_df['Id'] = id_col\n",
    "new_df['Predicted'] = final_preds\n",
    "print(new_df.describe())\n",
    "new_df.to_csv('../output/correlation_analysis_and_random_regress.csv', columns=['Id', 'Predicted'], index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T22:55:28.431923Z",
     "start_time": "2023-06-13T22:54:31.692522Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
