{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv('../dataset/development.csv')\n",
    "df_eval = pd.read_csv('../dataset/evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_preprocessing(df, scaler = None, dev_stats=None):\n",
    "\n",
    "    # one hot encoding\n",
    "    working_df = df.copy()\n",
    "    enc = OneHotEncoder()\n",
    "    encoded_df = pd.concat([working_df['weekday'], working_df['data_channel']], axis=1)\n",
    "    enc.fit(encoded_df)\n",
    "    encoded_df = enc.transform(encoded_df)\n",
    "    additional_columns = enc.get_feature_names_out()\n",
    "    working_df[additional_columns] = encoded_df.toarray()\n",
    "    # print(working_df.shape)\n",
    "\n",
    "    is_weekend = []\n",
    "    for _, row in working_df.iterrows():\n",
    "        if row['weekday_sunday'] == 1 or row['weekday_saturday'] == 1:\n",
    "            is_weekend.append(1)\n",
    "        else:\n",
    "            is_weekend.append(0)\n",
    "    working_df['is_weekend'] = is_weekend\n",
    "    working_df.drop(columns=[x for x in additional_columns if x.startswith('weekday')], inplace=True)\n",
    "\n",
    "    # feature selection from correlation analysis\n",
    "    working_df.drop(['weekday', 'data_channel', 'url', 'id', 'n_tokens_content', 'n_non_stop_words', 'kw_max_min',\n",
    "                'kw_min_max', 'kw_min_min', 'kw_max_avg', 'title_subjectivity', 'rate_positive_words'], axis = 1, inplace=True)\n",
    "    # , 'kw_avg_min', 'kw_avg_avg'\n",
    "    # 'url', 'id','weekday','data_channel'\n",
    "    working_df.drop(['self_reference_avg_sharess', 'kw_max_max', 'avg_positive_polarity', 'rate_negative_words', 'abs_title_sentiment_polarity', 'avg_negative_polarity', 'n_non_stop_unique_tokens'], inplace=True, axis=1)\n",
    "    \n",
    "    # fill missing values\n",
    "    working_df['num_keywords'].fillna(0, inplace=True)\n",
    "    working_df['num_imgs'].fillna(0, inplace=True)\n",
    "    working_df['num_self_hrefs'].fillna(0, inplace=True)\n",
    "    working_df['num_videos'].fillna(0, inplace=True)\n",
    "    if dev_stats == None:\n",
    "        dev_stats = dict()\n",
    "        kw_avg_min_mean =  working_df['kw_avg_min'][working_df['kw_avg_min']>0].mean()\n",
    "        kw_min_avg_mean =  working_df['kw_min_avg'][working_df['kw_min_avg']>0].mean()\n",
    "        working_df['kw_avg_min'] = working_df['kw_avg_min'].apply(lambda x: kw_avg_min_mean if x == -1 else x)\n",
    "        working_df['kw_min_avg'] = working_df['kw_min_avg'].apply(lambda x: kw_min_avg_mean if x == -1 else x)\n",
    "        dev_stats['kw_avg_min_mean'] = kw_avg_min_mean\n",
    "        dev_stats['kw_min_avg_mean'] = kw_min_avg_mean\n",
    "    else:\n",
    "        working_df['kw_avg_min'] = working_df['kw_avg_min'].apply(lambda x: dev_stats['kw_avg_min_mean'] if x == -1 else x)\n",
    "        working_df['kw_min_avg'] = working_df['kw_min_avg'].apply(lambda x: dev_stats['kw_avg_min_mean'] if x == -1 else x)\n",
    "    \n",
    "    \n",
    "    if scaler == None:\n",
    "        q1 = working_df['shares'].describe()['25%']\n",
    "        q3 = working_df['shares'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_shares = q1 - 1.5*iqr\n",
    "        max_shares = q3 + 1.5*iqr\n",
    "        # print(min_shares, max_shares)\n",
    "        working_df = working_df[(df.shares < max_shares) & (df.shares > min_shares)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['global_subjectivity'].describe()['25%']\n",
    "        q3 = working_df['global_subjectivity'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_global_subjectivity = q1 - 1.5*iqr\n",
    "        max_global_subjectivity = q3 + 1.5*iqr\n",
    "        # print(min_global_subjectivity, max_global_subjectivity)\n",
    "        working_df = working_df[(df.global_subjectivity < max_global_subjectivity) & (df.global_subjectivity > min_global_subjectivity)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['kw_avg_avg'].describe()['25%']\n",
    "        q3 = working_df['kw_avg_avg'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_kw_avg_avg = q1 - 1.5*iqr\n",
    "        max_kw_avg_avg = q3 + 1.5*iqr\n",
    "        # print(min_kw_avg_avg, max_kw_avg_avg)\n",
    "        working_df = working_df[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['self_reference_min_shares'].describe()['25%']\n",
    "        q3 = working_df['self_reference_min_shares'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_self_reference_min_shares = q1 - 1.5*iqr\n",
    "        max_self_reference_min_shares = q3 + 1.5*iqr\n",
    "        # print(min_self_reference_min_shares, max_self_reference_min_shares)\n",
    "        working_df = working_df[(df.self_reference_min_shares < max_self_reference_min_shares) & (df.self_reference_min_shares > min_self_reference_min_shares)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "        q1 = working_df['self_reference_max_shares'].describe()['25%']\n",
    "        q3 = working_df['self_reference_max_shares'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_self_reference_max_shares = q1 - 1.5*iqr\n",
    "        max_self_reference_max_shares = q3 + 1.5*iqr\n",
    "        # print(min_self_reference_min_shares, max_self_reference_min_shares)\n",
    "        working_df = working_df[(df.self_reference_max_shares < max_self_reference_max_shares) & (df.self_reference_max_shares > min_self_reference_max_shares)]\n",
    "        print(working_df.shape)\n",
    "\n",
    "\n",
    "        # q1 = working_df['num_self_hrefs'].describe()['25%']\n",
    "        # q3 = working_df['num_self_hrefs'].describe()['75%']\n",
    "        # iqr = q3 - q1\n",
    "        # min_num_self_hrefs = q1 - 1.5*iqr\n",
    "        # max_num_self_hrefs = q3 + 1.5*iqr\n",
    "        # # print(min_num_self_hrefs, max_num_self_hrefs)\n",
    "        # working_df = working_df[(df.num_self_hrefs < max_num_self_hrefs) & (df.num_self_hrefs > min_num_self_hrefs)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "        # q1 = working_df['num_imgs'].describe()['25%']\n",
    "        # q3 = working_df['num_imgs'].describe()['75%']\n",
    "        # iqr = q3 - q1\n",
    "        # min_num_imgs = q1 - 1.5*iqr\n",
    "        # max_num_imgs = q3 + 1.5*iqr\n",
    "        # # print(min_num_imgs, max_num_imgs)\n",
    "        # working_df = working_df[(df.num_imgs < max_num_imgs) & (df.num_imgs > min_num_imgs)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "        # q1 = working_df['num_videos'].describe()['25%']\n",
    "        # q3 = working_df['num_videos'].describe()['75%']\n",
    "        # iqr = q3 - q1\n",
    "        # min_num_videos = q1 - 1.5*iqr\n",
    "        # max_num_videos = q3 + 1.5*iqr\n",
    "        # # print(min_num_videos, max_num_videos)\n",
    "        # working_df = working_df[(df.num_videos < max_num_videos) & (df.num_videos > min_num_videos)]\n",
    "        # print(working_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # log scales\n",
    "    working_df['num_imgs'] = np.log(1 + working_df['num_imgs'])\n",
    "    working_df['num_self_hrefs'] = np.log(1 + working_df['num_self_hrefs'])\n",
    "    working_df['kw_avg_avg'] = np.log(1+working_df['kw_avg_avg'])\n",
    "    working_df['kw_avg_min'] =np.log(1+working_df['kw_avg_min'])\n",
    "    # avoid log on num_videos because it makes it worse\n",
    "\n",
    "    y_dev = None\n",
    "    \n",
    "    # standard scaler\n",
    "    if scaler == None:\n",
    "        print(working_df.shape)\n",
    "        # Remove outliers from kw_avg_avg (we lost another 9% of the dataset)\n",
    "        # q1 = working_df['kw_avg_avg'].describe()['25%']\n",
    "        # q3 = working_df['kw_avg_avg'].describe()['75%']\n",
    "        # iqr = q3 - q1\n",
    "        # min_kw_avg_avg = q1 - 1.5*iqr\n",
    "        # max_kw_avg_avg = q3 + 1.5*iqr\n",
    "        # working_df = working_df[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n",
    "\n",
    "        working_df['shares'] = np.log(working_df['shares'])\n",
    "        y_dev = working_df['shares']\n",
    "        working_df.drop(columns=['shares'], inplace=True)\n",
    "        scaler = StandardScaler().fit(working_df)\n",
    "        scaled_features = scaler.transform(working_df)\n",
    "        working_df[:] = scaled_features[:]\n",
    "\n",
    "        trans = RFECV(estimator=XGBRegressor(), step=1, cv=4 ,n_jobs=-1, verbose=2, scoring='neg_root_mean_squared_error')\n",
    "        trans.fit(working_df, y_dev)\n",
    "        dev_stats['trans'] = trans\n",
    "        working_df = trans.transform(working_df)\n",
    "        \n",
    "    else:\n",
    "        scaled_features = scaler.transform(working_df)\n",
    "        working_df[:] = scaled_features[:]\n",
    "        trans = dev_stats['trans']\n",
    "        working_df = trans.transform(working_df)\n",
    "\n",
    "    # print(scaled_features.shape)\n",
    "    working_df = working_df[:, [ 0,  1,  2,  3, 10, 11, 12, 14, 15, 29, 32, 34]]\n",
    "   \n",
    "\n",
    "    return working_df, scaler, y_dev, dev_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28081, 38)\n",
      "(26785, 38)\n",
      "(25591, 38)\n",
      "(22562, 38)\n",
      "(19924, 38)\n",
      "(19924, 38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_278866/1849703095.py:63: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.global_subjectivity < max_global_subjectivity) & (df.global_subjectivity > min_global_subjectivity)]\n",
      "/var/tmp/ipykernel_278866/1849703095.py:72: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.kw_avg_avg < max_kw_avg_avg) & (df.kw_avg_avg > min_kw_avg_avg)]\n",
      "/var/tmp/ipykernel_278866/1849703095.py:81: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.self_reference_min_shares < max_self_reference_min_shares) & (df.self_reference_min_shares > min_self_reference_min_shares)]\n",
      "/var/tmp/ipykernel_278866/1849703095.py:90: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  working_df = working_df[(df.self_reference_max_shares < max_self_reference_max_shares) & (df.self_reference_max_shares > min_self_reference_max_shares)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n"
     ]
    }
   ],
   "source": [
    "working_df_dev, std_scaler, y_dev, dev_stats = final_preprocessing(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = working_df_dev\n",
    "y = y_dev\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_stats['trans'].n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55439213566643\n",
      "0.17592776547602917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0.5588991495408469\\n0.18894985155235465\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg = GradientBoostingRegressor(**{'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': 4, 'min_samples_split': 2, 'n_estimators': 200, 'random_state': 42})\n",
    "rfreg.fit(X_train, y_train)\n",
    "\n",
    "rms = mean_squared_error(y_valid, rfreg.predict(X_valid), squared=False)\n",
    "print(rms)\n",
    "r2 = r2_score(y_valid, rfreg.predict(X_valid))\n",
    "adj_r2 = 1-(1-r2)*(len(X_valid) - 1)/(len(X_valid) - X_valid.shape[1] - 1)\n",
    "print(adj_r2)\n",
    "'''\n",
    "0.5588991495408469\n",
    "0.18894985155235465\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=200,\n",
       "                          random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=200,\n",
       "                          random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=200,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg = GradientBoostingRegressor(**{'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': 4, 'min_samples_split': 2, 'n_estimators': 200, 'random_state': 42})\n",
    "rfreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(rfreg.feature_importances_ > 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df_eval, _, _, _= final_preprocessing(df_eval, std_scaler, dev_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Id    Predicted\n",
      "count   7917.000000  7917.000000\n",
      "mean   35679.634584  1454.654358\n",
      "std     2289.051312   398.001047\n",
      "min    31715.000000   508.912630\n",
      "25%    33699.000000  1170.762520\n",
      "50%    35680.000000  1396.987923\n",
      "75%    37661.000000  1673.464539\n",
      "max    39643.000000  3878.277454\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfreg.predict(working_df_eval)\n",
    "final_preds = np.exp(y_pred)\n",
    "# Write CSV\n",
    "id_col = df_eval['id']\n",
    "new_df = pd.DataFrame(columns=['Id', 'Predicted'])\n",
    "new_df['Id'] = id_col\n",
    "new_df['Predicted'] = final_preds\n",
    "print(new_df.describe())\n",
    "new_df.to_csv('../output/gboost_with_rfecv.csv', columns=['Id','Predicted'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
