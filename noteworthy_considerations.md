## Dataset inspection

### General considerations
- The dataset has been acquired around 06/01/2015
- The LDA are clusters based on a topic.
- Global_subjectivity is a number between 0 and 1.
- Global_sentiment_polarity index for positive/neutral/negative sentiment of the text.
- Title_subjectivity is a number between 0 and 1. 
- Abs_title_subjectivity is a number between 0 and 0.5
- Example: Dato un set di parole chiave: ["a", "Giuseppe", "piace", "la", "pizza"] ad ognuna di queste può essere associata un numero di condivisioni in tutti i documenti e un valore che indica quanto fitta bene nel data channel o comunque nel contesto del testo. Ora la misura di quanto bene fitta nel testo è usata per definire worst or best word, il numero di articoli in cui queste parole compaiono di nuovo come keyword negli altri articoli determinano il numero di condivisioni (in alternativa si potrebbe pensare al numero di volte la parola è segnata come keyword nei metadata o non so dove altro). 


### Features descriptions
- URL: url of the article. It is not a useful feature for the predicting model because it is a sort of unique identifier of the article. In addition, the text of article can be retreived in order to use it in an words embedding but this implies the using of a pre-trained model. 
- Timedelta (numerical, ordinal, continuous in days): The number of days between the article publication and the dataset acquisition. This feature could be an interesting popularity (virality) indicator i.e. if timedelta is low and the number of corresponding shares is high, this means that the article became popular (viral) in a few days; this artifact is not always true because an article can take more days to become popular (viral).
- n_tokens_title (numerical, nominal, discrete in words): The number of words in the article's title. An initial assumption could be that a shorter title has more impact on the interest of the medium individual. From a preliminary comparison between the original title and this feature, the number does not match; it can be supposed that a stop-word elimination has been done in order to obtain the final counting. Nevertheless, some other mismatch may lead to additional processing before the count (e.g. a title inside the title). 
- n_tokens_content (numerical, nominal, discrete in words): The number of words in the article's content. It can be observed that there are some values equal to 0 words: this can be due to an error during the data collection step. A possible solution to overcome this error could be discretizing the articles based on this feature and then set the mean of the "low" bin as the final (filling) values of these articles. Another possible solution could be to get the HTML page content of these articles and set the actual number of words. An initial assumption could be the same as n_tokens_title. In addition, it is possible to observe that the number of words mismatch with respect to actual original content statistic. As for the previous feature, it can be supposed that a stop-word elimination step has been done before the count. 
- n_unique_tokens (numerical, ratio, continuous): The rate of unique words in the content. It is evaluated as: ${|unique_words| \over n_tokens_content}$. It is equal to 0 when $n_tokens_content$ is 0. 
- n_non_stop_words (numerical, ratio, continuous): The rate of non-stop-words in the content. It is evaluated as: ${|non_stop_words| \over n_tokens_content}$. The distribution of this feature is misleading because it is equal to 0 when $n\_tokens\_content$ is 0 and the rest of the values is around 100%. 
- n_non_stop_unique_tokens (numerical, ratio, continuous): The rate of unique non-stop words in the content. It is evaluated as: ${|unique\_non\_stop\_words| \over n\_tokens\_content}$. The variable is better distributed than the previous one and it is equal to 0 when $n\_tokens\_content$ is 0. 
- num_hrefs (numerical, ordinal, discrete): The number of external links. External means that link points out to another web-page which is not inside the Mashable domain. It is equal to 0 when $n\_tokens\_content$ is 0. The contrary does not hold because it is admissible that a news article does not have any external link. From a first comparison with original article it is possible to notice that there are some outliers in the data due to a possible error during data collection step. (e.g. there are 0 external links even though there is 1 or there are around 30 external links even though the record reports 304 external links)
- num_self_hrefs (numerical, ordinal, discrete): Number of links to other articles published by Mashable. It is equal to 0 when $n\_tokens\_content$ is 0. The contrary does not hold because it is admissible that a news article does not have any internal link. From a first comparison with original article it is possible to notice that there are some outliers in the data due to a possible error during data collection step (e.g. there are 0 internal links even though there are 2 or there are around 6 external links even though the record reports 116 internal links). 
- num_imgs (numerical, ordinal, discrete): Number of images in the article. This feature admits NaN values. Even though the value is NaN, the article may actually contain some images. A possible solution to overcome this problem is to set the NaN values to 1 because from an manual inspection, the article has 1 image, another reason could be searched in the fact that the articles where the reported number of images is 0, actually have 1 image that is the initial one. From a first comparison with original articles, it is possible to notice that there are inconsistent values of this feature, indeed some articles have reported only 1 image even though in the article there are 7 images. In addition, from the data distribution it is possibile to see that there are some outliers indeed the value until the 3th-Quartile is 4 images while the max value reported is 128 images.
- num_videos (numerical, ordinal, discrete): The number of videos. This feature admits NaN values. From a first comparison with original articles, the number of video reported in this feature does not correspond to the actual number of videos in the articles: even tough it is different from NaN, there is no videos inside a sample of articles. There is could be the possibility that the videos counting has been done by count the number of videos in the external links referenced by a Mashable's article. The data distribution of this feature shows that more than the majority of the articles do not contain any videos and some of them are considered as outliers.
- average_token_length (numerical, ratio, continuous): It is the average length of a word in terms of characters. It is evaluated as: ${|length_of_all_words| \over n_tokens_content}$. It is equal to 0 when $n_tokens_content$ is 0. Its values is not expressed in a range between [0, 1] because it is a mean based on the length of the words (so based on the number of characters of each word). As a first consideration, this is a first statistics based on the lenght of the words and, once a strategy to manage the "0" values related on the $n_tokens_content$ is found, this feature could be meaningful to predict the number of shares because people prefer to read texts with short words so that they are easier to read. 
- num_keywords (numerical, ordinal, discrete): The number of keywords in the metadata. This feature admits NaN values. The NaN values means that there is no keywords in the article's metadata (so it is possible to manage this NaN values by replace it with 0 integer value).
- data_channel (categorical, discrete): The type of data channel. This feature has only 6 unique values which are the category in which an article belongs. There is no Null values and the data distribution is quite balance but the "Social Media" channel.
- kw_min_min (numerical, ordinal, discrete): It is the minimum number of time that the worst keyword (in terms of fitness in the context or in the text) has been used in other documents (sharing). It does not have NaN values but with high probability the value "-1" has been used for this scope. These NaN values could be managed by computing the average of this feature for each data channel and replace the NaN values with it (based on the assigned data channel). When ${n_tokens_content}$ is 0, the value of this feature is 0 or it could be an invalid one. See example above.
- kw_max_min (numerical, ordinal, discrete): It is the maximum number of time that the worst keyword (in terms of fitness in the context or in the text) has been used in other documents (sharing). It does not have NaN values neither invalid values. It can be equal to 0 that means that the worst keyword has been used only in the current article. 
- kw_avg_min (numerical, ordinal, discrete): It is the maximum number of time that the worst keyword (in terms of fitness in the context or in the text) has been used in other documents (sharing). It does not have NaN values neither invalid values but with high probability the value "-1" has been used for this scope. I don't know between what the average is computed, maybe it is the number of shares is the number of time the keyword occurs in EACH other article.
- kw_min_max (numerical, ordinal, discrete)
...
- self_reference_min_shares (numerical, ordinal, discrete): Min. shares of referenced articles in Mashable. It does not contain any NaN value but most probably 0 values can be interpreted as NaN values.
- self_reference_max_shares (numerical, ordinal, discrete): Max. shares of referenced articles in Mashable. It does not contain any NaN value but most probably 0 values can be interpreted as NaN values.
- self_reference_avg_shares (numerical, ordinal, discrete): Avg. shares of referenced articles in Mashable. It does not contain any NaN value but most probably 0 values can be interpreted as NaN values.
- LDA_00 (numerical, rateo, continuous): Closeness to LDA topic 0 (Lifestyle). It has no NaN values but some 0 values can be mapped to NaN values. Only one record has this feature equal to 0 so I would not consider 0 as a NaN value. The record under analysis (id = 38792) belongs to the data channel "world".
- LDA_01 (numerical, rateo, continuous): Closeness to LDA topic 1 (Entertainment). It has no NaN values. Only one record has this feature equal to 0 so I would not consider 0 as a NaN value. The record under analysis (id = 38792) belongs to the data channel "world".
- LDA_02 (numerical, rateo, continuous): Closeness to LDA topic 2 (Business). It has no NaN values. Only one record has this feature equal to 0 so I would not consider 0 as a NaN value. The record under analysis (id = 38792) belongs to the data channel "world".
- LDA_03 (numerical, rateo, continuous): Closeness to LDA topic 3 (Social Media). It has no NaN values. Only one record has this feature equal to 0 so I would not consider 0 as a NaN value. The record under analysis (id = 38792) belongs to the data channel "world".
- LDA_04 (numerical, rateo, continuous): Closeness to LDA topic 4 (World). It has no NaN values. Only one record has this feature equal to 0 so I would not consider 0 as a NaN value. Even though the record belongs to the topic 4 it shows a LDA_04 score = 0. Then, since the record is the only which has such an irregularity i would delete it.
- global_subjectivity (numerical, rateo, continuous): an indicator of text subjectivity. Without considering 3 records which except this rule, when the ${n\_tokens\_content}$ equals 0, this feature assume value equal to 0. 
- global_sentiment_polarity (numerical, rateo, continuous): an indicator of the text sentiment. The more positive the article, the higher is this value. The feature does not present any NaN value. 
- global_rate_positive_words (numerical, rateo, continuous): The rate of words with positive sentiment in the content.It is computed as: ${#positive\_words \over n\_tokens\_content}$. Zero values are admissile. 
- global_rate_negative_words (numerical, rateo, continuous): The rate of words with negative sentiment in the content.It is computed as: ${#negative\_words \over n\_tokens\_content}$. Zero values are admissile. 
- rate_positive_words (numerical, rateo, continuous): The rate of words with positive sentiment between the words where the sentiment class is different "neutral". It is computed as: ${#positive\_words \over (#positive\_words + #negative\_words)}$. Zero values are admissile and of course, they highly depend on the global_rate_positive_words variable.
- rate_negative_words (numerical, rateo, continuous): The rate of words with negative sentiment between the words where the sentiment class is different "neutral". It is computed as: ${#negative\_words \over (#positive\_words + #negative\_words)}$. Zero values are admissile and of course, they highly depend on the global_rate_negative_words variable.
- avg_positive_polarity (numerical, rateo, continuous): Average polarity of positive words. It is computed as ${sum(polarity_of_positive_words) \over #positive\_words }$. It admits 0 values, when they occour, the feature global_rate_positive_words, min_positive_polarity and max_positive_polarity are reasonably always 0. It goes from 0 to 1.
- min_positive_polarity (numerical, rateo, continuous): Minimum polarity of positive words. It admits 0 values, when they occour, the feature global_rate_positive_words, avg_positive_polarity and rate_positive_words are reasonably always 0. It goes from 0 to 1.
- max_positive_polarity (numerical, rateo, continuous): Maximum polarity of positive words. It admits 0 values, when they occour, the feature global_rate_positive_words, avg_positive_polarity and rate_positive_words are reasonably always 0. It goes from 0 to 1.
- avg_negative_polarity (numerical, rateo, continuous): Average polarity of positive words. It is computed as ${sum(polarity_of_positive_words) \over #positive\_words }$. It admits 0 values, when they occour, the feature global_rate_positive_words, min_positive_polarity and max_positive_polarity are reasonably always 0. It goes from 0 to 1.
- min_negative_polarity (numerical, rateo, continuous): Minimum polarity of negative words. It admits 0 values, when they occour, the feature global_rate_negative_words, avg_negative_polarity and rate_negative_words are reasonably always 0. It goes from 0 to 1.
- max_negative_polarity (numerical, rateo, continuous):
- title_subjectivity (numerical, rateo, continuous):
- title_sentiment_polarity (numerical, rateo, continuous):
- abs_title_subjectivity ():
- abs_title_sentiment_polarity (): 
- shares (numerical, ordinal, discrete): 





