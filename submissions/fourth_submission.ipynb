{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "import xgboost as xg\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv('../dataset/development.csv')\n",
    "df_eval = pd.read_csv('../dataset/evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_preprocessing(df, reduce_df=True):\n",
    "    df_preproc = df.copy()\n",
    "\n",
    "    # one hot encoding\n",
    "    enc = OneHotEncoder()\n",
    "    encoded_df = pd.concat([df_preproc['weekday'], df_preproc['data_channel']], axis=1)\n",
    "    enc.fit(encoded_df)\n",
    "    encoded_df = enc.transform(encoded_df)\n",
    "    additional_columns = enc.get_feature_names_out()\n",
    "    print(encoded_df.toarray().shape)\n",
    "    df_preproc[additional_columns] = encoded_df.toarray()\n",
    "    df_preproc.drop(['weekday', 'data_channel', 'url', 'id'], axis = 1, inplace=True)\n",
    "\n",
    "    # drop from feature selection\n",
    "    df_preproc.drop(columns=['n_unique_tokens','n_non_stop_words','kw_max_min','kw_min_max','kw_max_avg','abs_title_sentiment_polarity',\n",
    "                     'abs_title_subjectivity','rate_positive_words','timedelta','max_negative_polarity','min_negative_polarity',\n",
    "                     'kw_min_min','kw_max_max','num_self_hrefs','data_channel_bus','LDA_00'], inplace=True)\n",
    "    # reduce df\n",
    "    if reduce_df:\n",
    "        df_preproc = df_preproc.query(\"n_tokens_content > 0\")\n",
    "        # Remove outliers from kw_avg_avg (we lost another 9% of the dataset)\n",
    "        q1 = df_preproc['kw_avg_avg'].describe()['25%']\n",
    "        q3 = df_preproc['kw_avg_avg'].describe()['75%']\n",
    "        iqr = q3 - q1\n",
    "        min_kw_avg_avg = q1 - 1.5*iqr\n",
    "        max_kw_avg_avg = q3 + 1.5*iqr\n",
    "        df_preproc = df_preproc[(df_preproc.kw_avg_avg < max_kw_avg_avg) & (df_preproc.kw_avg_avg > min_kw_avg_avg)]\n",
    "    \n",
    "    std_scaler = StandardScaler().fit(df_preproc[['n_tokens_content']])\n",
    "    scaled_features = std_scaler.transform(df_preproc[['n_tokens_content']])\n",
    "    df_preproc[['n_tokens_content']] = scaled_features\n",
    "    \n",
    "    df_preproc['num_imgs'].fillna(df_preproc['num_imgs'].mean(), inplace=True)\n",
    "    # df_preproc['num_imgs'] = np.log(1+df_preproc['num_imgs'])\n",
    "    std_scaler = StandardScaler().fit(df_preproc[['num_imgs']])\n",
    "    scaled_features = std_scaler.transform(df_preproc[['num_imgs']])\n",
    "    df_preproc[['num_imgs']] = scaled_features\n",
    "\n",
    "    df_preproc['num_videos'].fillna(df_preproc['num_videos'].mean(), inplace=True)\n",
    "    std_scaler = StandardScaler().fit(df_preproc[['num_videos']])\n",
    "    scaled_features = std_scaler.transform(df_preproc[['num_videos']])\n",
    "    df_preproc[['num_videos']] = scaled_features\n",
    "    \n",
    "    std_scaler = StandardScaler().fit(df_preproc[['n_tokens_title']])\n",
    "    scaled_features = std_scaler.transform(df_preproc[['n_tokens_title']])\n",
    "    df_preproc[['n_tokens_title']] = scaled_features\n",
    "\n",
    "    if 'shares' in df_preproc.columns:\n",
    "        df_preproc['shares'] = np.log(df_preproc['shares'])\n",
    "    \n",
    "    df_preproc.drop(columns=['self_reference_min_shares','self_reference_max_shares', 'self_reference_avg_sharess'], inplace = True)\n",
    "\n",
    "    df_preproc.drop(columns=['LDA_01', 'LDA_02', 'LDA_03', 'LDA_04'], inplace=True)\n",
    "\n",
    "    is_weekend = []\n",
    "    for _, row in df_preproc.iterrows():\n",
    "        if row['weekday_sunday'] == 1 or row['weekday_saturday'] == 1:\n",
    "            is_weekend.append(1)\n",
    "        else:\n",
    "            is_weekend.append(0)\n",
    "    df_preproc['is_weekend'] = is_weekend\n",
    "\n",
    "\n",
    "    df_preproc.drop(columns=[x for x in additional_columns if x.startswith('weekday')], inplace=True)\n",
    "\n",
    "    new_df_keywords = df.copy()\n",
    "    df_preproc['num_keywords'] = new_df_keywords.groupby(['data_channel'], sort=False)['num_keywords'].apply(lambda x: x.fillna(x.mean())).reset_index()['num_keywords']\n",
    "\n",
    "    std_scaler = StandardScaler().fit(df_preproc[['num_keywords']])\n",
    "    scaled_features = std_scaler.transform(df_preproc[['num_keywords']])\n",
    "    df_preproc[['num_keywords']] = scaled_features\n",
    "\n",
    "    df_preproc.drop(columns=['kw_avg_min'], inplace=True)\n",
    "    std_scaler = StandardScaler().fit(df_preproc[['kw_avg_max', 'kw_min_avg', 'kw_avg_avg']])\n",
    "    scaled_features = std_scaler.transform(df_preproc[['kw_avg_max', 'kw_min_avg', 'kw_avg_avg']])\n",
    "    df_preproc[['kw_avg_max', 'kw_min_avg', 'kw_avg_avg']] = scaled_features\n",
    "\n",
    "\n",
    "    # df_preproc['avg_negative_polarity'] = df_preproc['avg_negative_polarity'].abs()\n",
    "\n",
    "    return df_preproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31715, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_451009/4050322526.py:67: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_preproc['num_keywords'] = new_df_keywords.groupby(['data_channel'], sort=False)['num_keywords'].apply(lambda x: x.fillna(x.mean())).reset_index()['num_keywords']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>data_channel_entertainment</th>\n",
       "      <th>data_channel_lifestyle</th>\n",
       "      <th>data_channel_socmed</th>\n",
       "      <th>data_channel_tech</th>\n",
       "      <th>data_channel_world</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.766514</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.545031</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.902772</td>\n",
       "      <td>-0.066116</td>\n",
       "      <td>4.656158</td>\n",
       "      <td>-1.845251</td>\n",
       "      <td>1.129960</td>\n",
       "      <td>0.698238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.972466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.649809</td>\n",
       "      <td>-0.133735</td>\n",
       "      <td>0.737542</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.066116</td>\n",
       "      <td>4.576541</td>\n",
       "      <td>1.618213</td>\n",
       "      <td>-1.027350</td>\n",
       "      <td>-0.965816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.170120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.649809</td>\n",
       "      <td>-0.715458</td>\n",
       "      <td>0.748428</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.046128</td>\n",
       "      <td>-0.066116</td>\n",
       "      <td>4.935345</td>\n",
       "      <td>-0.690763</td>\n",
       "      <td>2.480468</td>\n",
       "      <td>-0.087832</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.427500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.781320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.766514</td>\n",
       "      <td>-0.846399</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.590804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.970760</td>\n",
       "      <td>-0.690763</td>\n",
       "      <td>0.458149</td>\n",
       "      <td>0.944244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>7.313220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.294406</td>\n",
       "      <td>-0.599543</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.337313</td>\n",
       "      <td>5.006993</td>\n",
       "      <td>0.338227</td>\n",
       "      <td>-1.882379</td>\n",
       "      <td>-0.965816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251786</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>7.244228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31709</th>\n",
       "      <td>0.294406</td>\n",
       "      <td>-0.101536</td>\n",
       "      <td>0.703008</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.454635</td>\n",
       "      <td>-0.066116</td>\n",
       "      <td>4.372587</td>\n",
       "      <td>0.266477</td>\n",
       "      <td>-0.231487</td>\n",
       "      <td>2.207003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386310</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>6.527958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31710</th>\n",
       "      <td>0.294406</td>\n",
       "      <td>-0.268969</td>\n",
       "      <td>0.718978</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.784091</td>\n",
       "      <td>-1.268007</td>\n",
       "      <td>0.038879</td>\n",
       "      <td>0.502769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.907755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31712</th>\n",
       "      <td>-0.649809</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.710623</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.318466</td>\n",
       "      <td>-0.066116</td>\n",
       "      <td>4.594427</td>\n",
       "      <td>0.463725</td>\n",
       "      <td>-0.312466</td>\n",
       "      <td>-0.965816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.783224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31713</th>\n",
       "      <td>0.294406</td>\n",
       "      <td>3.028178</td>\n",
       "      <td>0.621080</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-0.454635</td>\n",
       "      <td>0.476277</td>\n",
       "      <td>4.353239</td>\n",
       "      <td>1.618213</td>\n",
       "      <td>0.187219</td>\n",
       "      <td>1.580256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323413</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>8.699515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31714</th>\n",
       "      <td>1.238621</td>\n",
       "      <td>-0.653207</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.454635</td>\n",
       "      <td>-0.066116</td>\n",
       "      <td>4.279693</td>\n",
       "      <td>-0.338474</td>\n",
       "      <td>0.706407</td>\n",
       "      <td>1.092306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206019</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>7.313220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29450 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_non_stop_unique_tokens  num_hrefs  \\\n",
       "0            0.766514          0.965314                  0.545031       10.0   \n",
       "1           -0.649809         -0.133735                  0.737542        9.0   \n",
       "2           -0.649809         -0.715458                  0.748428       12.0   \n",
       "3            0.766514         -0.846399                  0.867925        9.0   \n",
       "4            0.294406         -0.599543                  0.800000        5.0   \n",
       "...               ...               ...                       ...        ...   \n",
       "31709        0.294406         -0.101536                  0.703008        9.0   \n",
       "31710        0.294406         -0.268969                  0.718978       10.0   \n",
       "31712       -0.649809          0.866572                  0.710623        6.0   \n",
       "31713        0.294406          3.028178                  0.621080       21.0   \n",
       "31714        1.238621         -0.653207                  0.780822        5.0   \n",
       "\n",
       "       num_imgs  num_videos  average_token_length  num_keywords  kw_avg_max  \\\n",
       "0      3.902772   -0.066116              4.656158     -1.845251    1.129960   \n",
       "1      0.000000   -0.066116              4.576541      1.618213   -1.027350   \n",
       "2     -0.046128   -0.066116              4.935345     -0.690763    2.480468   \n",
       "3     -0.590804    0.000000              4.970760     -0.690763    0.458149   \n",
       "4      0.000000   -0.337313              5.006993      0.338227   -1.882379   \n",
       "...         ...         ...                   ...           ...         ...   \n",
       "31709 -0.454635   -0.066116              4.372587      0.266477   -0.231487   \n",
       "31710  0.000000    0.000000              4.784091     -1.268007    0.038879   \n",
       "31712 -0.318466   -0.066116              4.594427      0.463725   -0.312466   \n",
       "31713 -0.454635    0.476277              4.353239      1.618213    0.187219   \n",
       "31714 -0.454635   -0.066116              4.279693     -0.338474    0.706407   \n",
       "\n",
       "       kw_min_avg  ...  avg_negative_polarity  title_subjectivity  \\\n",
       "0        0.698238  ...              -0.160714            0.000000   \n",
       "1       -0.965816  ...              -0.157500            0.000000   \n",
       "2       -0.087832  ...              -0.427500            0.000000   \n",
       "3        0.944244  ...              -0.216667            0.400000   \n",
       "4       -0.965816  ...              -0.251786            0.200000   \n",
       "...           ...  ...                    ...                 ...   \n",
       "31709    2.207003  ...              -0.386310            0.288889   \n",
       "31710    0.502769  ...              -0.209167            0.000000   \n",
       "31712   -0.965816  ...              -0.400000            0.000000   \n",
       "31713    1.580256  ...              -0.323413            0.700000   \n",
       "31714    1.092306  ...              -0.206019            0.325000   \n",
       "\n",
       "       title_sentiment_polarity    shares  data_channel_entertainment  \\\n",
       "0                      0.000000  7.972466                         0.0   \n",
       "1                      0.000000  7.170120                         0.0   \n",
       "2                      0.000000  9.781320                         0.0   \n",
       "3                     -0.250000  7.313220                         0.0   \n",
       "4                     -0.100000  7.244228                         0.0   \n",
       "...                         ...       ...                         ...   \n",
       "31709                 -0.155556  6.527958                         0.0   \n",
       "31710                  0.000000  6.907755                         0.0   \n",
       "31712                  0.000000  7.783224                         0.0   \n",
       "31713                 -0.400000  8.699515                         0.0   \n",
       "31714                  0.175000  7.313220                         0.0   \n",
       "\n",
       "       data_channel_lifestyle  data_channel_socmed  data_channel_tech  \\\n",
       "0                         0.0                  0.0                0.0   \n",
       "1                         0.0                  0.0                1.0   \n",
       "2                         1.0                  0.0                0.0   \n",
       "3                         0.0                  0.0                0.0   \n",
       "4                         0.0                  0.0                1.0   \n",
       "...                       ...                  ...                ...   \n",
       "31709                     1.0                  0.0                0.0   \n",
       "31710                     0.0                  0.0                0.0   \n",
       "31712                     0.0                  0.0                1.0   \n",
       "31713                     1.0                  0.0                0.0   \n",
       "31714                     0.0                  1.0                0.0   \n",
       "\n",
       "       data_channel_world  is_weekend  \n",
       "0                     0.0           0  \n",
       "1                     0.0           0  \n",
       "2                     0.0           0  \n",
       "3                     0.0           0  \n",
       "4                     0.0           0  \n",
       "...                   ...         ...  \n",
       "31709                 0.0           0  \n",
       "31710                 1.0           0  \n",
       "31712                 0.0           0  \n",
       "31713                 0.0           0  \n",
       "31714                 0.0           0  \n",
       "\n",
       "[29450 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df_dev = final_preprocessing(df_dev)\n",
    "working_df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = working_df_dev.drop(columns=[\"shares\"]).values\n",
    "y = working_df_dev[\"shares\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_samples_split\": 3,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gesposito/project/predict-shares-online-news/submissions/fourth_submission.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blegionlogin.polito.it/home/gesposito/project/predict-shares-online-news/submissions/fourth_submission.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m reg \u001b[39m=\u001b[39m GradientBoostingRegressor(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\u001b[39m.\u001b[39;49mfit(X,y)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resize_state()\n\u001b[1;32m    537\u001b[0m \u001b[39m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m n_stages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_stages(\n\u001b[1;32m    539\u001b[0m     X,\n\u001b[1;32m    540\u001b[0m     y,\n\u001b[1;32m    541\u001b[0m     raw_predictions,\n\u001b[1;32m    542\u001b[0m     sample_weight,\n\u001b[1;32m    543\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rng,\n\u001b[1;32m    544\u001b[0m     X_val,\n\u001b[1;32m    545\u001b[0m     y_val,\n\u001b[1;32m    546\u001b[0m     sample_weight_val,\n\u001b[1;32m    547\u001b[0m     begin_at_stage,\n\u001b[1;32m    548\u001b[0m     monitor,\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    551\u001b[0m \u001b[39m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m n_stages \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    608\u001b[0m     old_oob_score \u001b[39m=\u001b[39m loss_(\n\u001b[1;32m    609\u001b[0m         y[\u001b[39m~\u001b[39msample_mask],\n\u001b[1;32m    610\u001b[0m         raw_predictions[\u001b[39m~\u001b[39msample_mask],\n\u001b[1;32m    611\u001b[0m         sample_weight[\u001b[39m~\u001b[39msample_mask],\n\u001b[1;32m    612\u001b[0m     )\n\u001b[1;32m    614\u001b[0m \u001b[39m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m raw_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_stage(\n\u001b[1;32m    616\u001b[0m     i,\n\u001b[1;32m    617\u001b[0m     X,\n\u001b[1;32m    618\u001b[0m     y,\n\u001b[1;32m    619\u001b[0m     raw_predictions,\n\u001b[1;32m    620\u001b[0m     sample_weight,\n\u001b[1;32m    621\u001b[0m     sample_mask,\n\u001b[1;32m    622\u001b[0m     random_state,\n\u001b[1;32m    623\u001b[0m     X_csc,\n\u001b[1;32m    624\u001b[0m     X_csr,\n\u001b[1;32m    625\u001b[0m )\n\u001b[1;32m    627\u001b[0m \u001b[39m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:257\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    254\u001b[0m     sample_weight \u001b[39m=\u001b[39m sample_weight \u001b[39m*\u001b[39m sample_mask\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    256\u001b[0m X \u001b[39m=\u001b[39m X_csr \u001b[39mif\u001b[39;00m X_csr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\n\u001b[0;32m--> 257\u001b[0m tree\u001b[39m.\u001b[39;49mfit(X, residual, sample_weight\u001b[39m=\u001b[39;49msample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    259\u001b[0m \u001b[39m# update tree leaves\u001b[39;00m\n\u001b[1;32m    260\u001b[0m loss\u001b[39m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    261\u001b[0m     tree\u001b[39m.\u001b[39mtree_,\n\u001b[1;32m    262\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     k\u001b[39m=\u001b[39mk,\n\u001b[1;32m    270\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/sklearn/tree/_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1219\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1247\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   1248\u001b[0m         X,\n\u001b[1;32m   1249\u001b[0m         y,\n\u001b[1;32m   1250\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1251\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m   1252\u001b[0m     )\n\u001b[1;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reg = GradientBoostingRegressor(**params).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7917, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_451009/4050322526.py:67: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_preproc['num_keywords'] = new_df_keywords.groupby(['data_channel'], sort=False)['num_keywords'].apply(lambda x: x.fillna(x.mean())).reset_index()['num_keywords']\n"
     ]
    }
   ],
   "source": [
    "working_df_eval = final_preprocessing(df_eval, reduce_df=False)\n",
    "X_test = working_df_eval.values\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "final_preds = np.exp(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = df_eval['id']\n",
    "new_df = pd.DataFrame(columns=['Id', 'Predicted'])\n",
    "new_df['Id'] = id_col\n",
    "new_df['Predicted'] = final_preds\n",
    "new_df.to_csv('../output.csv', columns=['Id','Predicted'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbr = XGBRegressor(n_estimators=500, learning_rate=0.01, eval_metric = 'rmsle', alpha=0.5)\n",
    "# xgb_r = xgbr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7917, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_451009/4050322526.py:67: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_preproc['num_keywords'] = new_df_keywords.groupby(['data_channel'], sort=False)['num_keywords'].apply(lambda x: x.fillna(x.mean())).reset_index()['num_keywords']\n"
     ]
    }
   ],
   "source": [
    "working_df_eval = final_preprocessing(df_eval, reduce_df=False)\n",
    "X_test = working_df_eval.values\n",
    "\n",
    "# y_pred = xgb_r.predict(X_test)\n",
    "final_preds = np.exp(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = df_eval['id']\n",
    "new_df = pd.DataFrame(columns=['Id', 'Predicted'])\n",
    "new_df['Id'] = id_col\n",
    "new_df['Predicted'] = final_preds\n",
    "new_df.to_csv('../output.csv', columns=['Id','Predicted'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
